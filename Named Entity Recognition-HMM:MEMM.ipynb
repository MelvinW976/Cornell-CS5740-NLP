{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZNdFlbLJoUXM"
      },
      "source": [
        "# Homework 1: Named Entity Recognition (NER) with Sequence Labeling Models\n",
        "## CS4740/5740 Fall 2022\n",
        "\n",
        "### Milestone Submission Due: **September 15, 2022 (11:59 PM)** \n",
        "\n",
        "### Project Submission Due: **September 27, 2022 (11:59 PM)**\n",
        "\n",
        "Should there be major updates to this document, we will announce them on [Ed Stem](https://edstem.org/us/courses/26793/discussion/).  Minor updates are documented on [this particular much-beloved thread](https://edstem.org/us/courses/26793/discussion/1739296).\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "871f2XhgtoYX"
      },
      "source": [
        "**Names:**\n",
        "Menglin Wang, Zhejun He\n",
        "**Netids:**\n",
        "mw976, zh254\n",
        "\n",
        "\n",
        "\n",
        "**Editing your version of this notebook:** One partner should make a copy of this notebook and share it with your partner.  **However**, because of synchronization issues (even though Colab works with Google Drive), changes made in this notebook at the same time from different computers/browser windows may not save. We will go so far as to recommend that you close the tab with this notebook when you are not working on it so your partner doesn't face sync issues.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QN0nBtOzb5u7"
      },
      "source": [
        "**Collaboration policy:** please be sure to check the collaboration policy on the [course website](https://courses.cs.cornell.edu/cs4740/2022fa/)!\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xY4p6eJmPPY4"
      },
      "source": [
        "> Assignment authors & testers: CS 4740/5740 professors and TAs from this and previous semesters, Chenxin Fang, Meghana Srivastava, Khonzoda Umarova, as well as Ruizhe Wang, Han Xia, and Heather Zhang."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iguUiw0mor52"
      },
      "source": [
        "# **Introduction**\n",
        "---\n",
        "\n",
        "In this project, you will tackle the **Named Entity Recognition** task: you'll implement models that identify named entities in text and tag them with the appropriate label. A primer on this task is provided further on.  We will treat this as a **sequence-tagging task**: for each token in the input text, assign one of the following 5 entity labels -- **ORG** (Organization), **PER** (Person), **LOC** (Location), **MISC** (Miscellaneous), and **O** (Not Named Entity) -- as well as a BIO-format prefix **B-** (token is the *beginning* of a named entity) or **I-** (token is *inside* a named entity). Overall, this yields 9 different labels: **B-ORG, I-ORG, B-PER, I-PER, B-LOC, I-LOC, B-MISC, I-MISC** and **O**.\n",
        "\n",
        "For this project, you will implement two sequence labeling approaches:\n",
        "- Model 1 : a Hidden Markov Model (HMM)\n",
        "- Model 2 : a Maximum Entropy Markov Model (MEMM)/Logistic Regression classifier (also known as a MaxEnt classifier). Feature engineering is strongly suggested for this model!\n",
        "\n",
        "A key component of both models is implementation of the Viterbi algorithm, which we will use to find the most likely tag sequence to assign to an input text."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mib4pTXj3hir"
      },
      "source": [
        "## **Logistics**\n",
        "\n",
        "---\n",
        "\n",
        "- You are **strongly encouraged** to work in **groups of 2 students**. Students in the same group will get the same grade. Thus, you should make sure that everyone in your group contributes to the project. \n",
        "- **Do not form teams of two on Kaggle** (You *will* form teams on a different platform; details TBA) because submitting separately gives you more tries, which is useful when experimenting with different models. \n",
        "- **Before submitting your predictions on Kaggle**, please rename your \"Kaggle team\" (since you and your partner are not forming your group on the Kaggle platform, this \"team\" would actually only include you) to be your NetID (eg: `team_ku47`). You can do so under `Team` tab of the competition. \n",
        "- A part of your submission would involve uploading your notebook (details would be provided soon!). So, please enter all code and answer all the questions in this colab notebook.\n",
        "  - Your code must have docstrings/comments documenting the meaning of parameters and important parameter-like variables.\n",
        "- In this assignment you are asked to make two submissions:\n",
        "  1. Intermediate **milestone submission due on 9/15/22 (11:59 PM)**. For this, please 1) have your teams formed (mechanism TBA) and  2) submit predictions of your first model (HMM) on Kaggle. This means that you should aim to complete Part 1 and Part 2 of the assignment by this milestone. Points will be awarded for meeting the milestone deadline, but we will only grade for completion, not correctness. \n",
        "  2. The **final homework submission due on 9/27/22 (11:59 PM)**. (details TBA)\n",
        "- Please be sure to consult the [list](https://docs.google.com/document/d/1-QmpkZYJDCM4gQQ9sZW2EwD5ltYy1CFjlJTbhD0Be-A/edit?usp=sharing) of banned packages/libraries before you start implementing your models. Note that this list may get updated.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q2z7TIHV3kCH"
      },
      "source": [
        "## **Advice**\n",
        "\n",
        "---\n",
        "\n",
        "1. Please read through the entire notebook before you start coding. That might inform your code structure.\n",
        "2. An assignment outline and grading breakdown (subject to minor adjustments) is found below; please consult it.\n",
        "3. The project is somewhat open ended. We will ask you to implement the models, but in some cases precise data structures and so on can be chosen by you. However, to integrate with Kaggle, you will need to submit Kaggle predictions using the given evaluation code (more instructions later)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NyokCzP2Zqqx"
      },
      "source": [
        "<a name=\"outline\"></a>\n",
        "## **Assignment outline and grading breakdown**\n",
        "- [Part 1](#part1)\n",
        "  - [Q1](#q1) [10 pts]\n",
        "- [Part 2](#part2)\n",
        "  - [Unknown Word Handling](#unknowns_handling) [15 pts]\n",
        "  - [HMM Implementation](#hmm_implementation) [20 pts]\n",
        "  - [Viterbi Implementation](#viterbi_implementation) [20 pts]\n",
        "  - [Validation](#validation_data) [3 pts]\n",
        "  - [Q2.1](#q2.1) [5 pts]\n",
        "  - [Q2.2](#q2.2) [5 pts]\n",
        "  - [Q2.3](#q2.3) [5 pts]\n",
        "  - [Q2.4](#q2.4) [5 pts]\n",
        "- [Part 3](#part3)\n",
        "  - [Features](#features) [35 pts]\n",
        "  - [MEMM Implementation](#memm_implementation) [25 pts]\n",
        "  - [Q3.1](#q3.1) [5 pts]\n",
        "  - [Q3.2](#q3.2) [5 pts]\n",
        "  - [Q3.3](#q3.3) [5 pts]\n",
        "  - [Q3.4](#q3.4) [5 pts]\n",
        "- [Part 4](#part4)\n",
        "  - [Q4.1](#q4.1) [7 pts]\n",
        "  - [Q4.2](#q4.2) [7 pts]\n",
        "  - [Q4.3](#q4.3) [7 pts]\n",
        "- [Part 5](#part5)\n",
        "  - [Q5](#q5)\n",
        "\n",
        "\n",
        "Meeting the milestone deadline [10 pts];\n",
        "\n",
        "Outperforming our baseline on Kaggle [15 pts];\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kBsjyfPu5V4t"
      },
      "source": [
        "## **Named Entity Recognition: Review**\n",
        "\n",
        "---\n",
        "\n",
        "NER refers to the information extraction technique of identifying and categorizing key information about entities within textual data. NER is important for:\n",
        "  - Detecting entities in search engines and voice assistants for more relavent search results.\n",
        "  - Automatically parsing resumes.\n",
        "  - ...and much more!\n",
        "\n",
        "In our dataset named entity tags are formatted in BIO/IOB format. With this format, entity tags get a prefix. Prefix \"B-\" is added to the first word/token of the entity name. All following tokens that are part of the same entity name would get prefix \"I-\". \n",
        "\n",
        "Here is an example sentence: \"ZIFA\n",
        "said\n",
        "Renate\n",
        "Geotschel\n",
        "of\n",
        "Austria\n",
        "won the women's World Cup  downhill race in Germany.\"\n",
        "Entity \"Renate Goetschl\" gets \"Renate\" (B-PER) and \"Goestchl\" (I-PER). Similarly, for \"World Cup\" we'd have \"World\" (B-MISC) and \"Cup\" (I-MISC). If an entity only has one token, then its entity tag would still have prefix \"B-\". \"O\" is used to denote tokens that are not part of any named entity. Thus, from the example above, we'd have:\n",
        "\n",
        "```\"ZIFA\" B-ORG```\n",
        "\n",
        " ```\"said\" O```\n",
        "\n",
        " ```\"Renate\" B-PER```\n",
        "\n",
        " ```\"Goetschl\" I-PER```\n",
        "\n",
        " ```\"of\" O```\n",
        "\n",
        " ```\"Austria\" B-LOC```\n",
        "\n",
        " ```\"won\" O```\n",
        "\n",
        " ```\"the\" O```\n",
        "\n",
        " ```\"women's\" O```\n",
        "\n",
        " ```\"World\" B-MISC```\n",
        "\n",
        " ```\"Cup\" I-MISC```\n",
        "\n",
        " ```\"downhill\" O```\n",
        "\n",
        " ```\"race\" O```\n",
        "\n",
        " ```\"in\" O```\n",
        "\n",
        " ```\"Germany\" B-LOC```\n",
        "\n",
        "\n",
        "Although NER is predominantly handled by deep learning approaches, for now let's use HMMs and MEMMs. \n",
        "\n",
        "\n",
        "To read more on NER, we refer to any of the following sources:\n",
        "1. Medium post [1](https://umagunturi789.medium.com/everything-you-need-to-know-about-named-entity-recognition-2a136f38c08f) and [2](https://medium.com/mysuperai/what-is-named-entity-recognition-ner-and-how-can-i-use-it-2b68cf6f545d).\n",
        "2. Try out [this](https://demo.allennlp.org/named-entity-recognition/named-entity-recognition) AlllenNLP demo! Please note that this demo uses a slightly different format of NER tags."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DSFfegs8LKY8"
      },
      "source": [
        "## **Evaluation: Entity Level Mean F1**\n",
        "\n",
        "---\n",
        "\n",
        "The standard evaluation measures to report for NER are recall, precision, and F1 score\n",
        "(also called F-measure) evaluated at the **named entity level** (not at the token level). The code for this has been provided later under the validation section under Part 2. Please use this code when evaluating your models. \n",
        "\n",
        "\n",
        "If P and T are the sets of predicted and true *named entity spans*, respectively, (e.g, the five named entity spans in the above example are \"Zifa\", \"Renate Goetschl\", \"Austria\", \"World Cup\", and \"Germany\") then\n",
        "\n",
        "####<center>Precision = $\\frac{|\\text{P}\\;\\cap\\;\\text{T}|}{|\\text{P}|}$ and Recall = $\\frac{|\\text{P}\\;\\cap\\;\\text{T}|}{|\\text{T}|}$.</center><br/>\n",
        "\n",
        "\n",
        "####<center>F1 = $\\frac{2 * \\text{Precision} * \\text{Recall}}{\\text{Precision} + \\text{Recall}}$. </center><br/>\n",
        "\n",
        "For each type of named entity, e.g. *LOC*ation, *MISC*ellaneous, *ORG*anization and *PER*son, we calculate the F1 score as shown above, and take the mean of all these F1 scores to get the **Entity Level Mean F1** score for the test set. If $N$ is the total number of labels (i.e., named entity types), then\n",
        "\n",
        "####<center>Entity Level Mean F1 = $\\frac{\\sum_{i = 1}^{N} \\text{F1}_{{label}_i}}{N}$. </center>\n",
        "\n",
        "More details under the validation section in Part 2.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7iP63fHj5saG"
      },
      "source": [
        "<a name=\"part1\"></a>\n",
        "# **Part 1: Dataset**\n",
        "[[^^^]](#outline) \n",
        "\n",
        "Our data is a modified version of the WikiNEuRal ([ Tedeschi et al.](https://aclanthology.org/2021.findings-emnlp.215.pdf)) dataset."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pljkH2ow5U9x"
      },
      "source": [
        "Load the dataset as follows:\n",
        "  1. Obtain the data from Data tab of the [Kaggle competition](https://www.kaggle.com/t/200697e4726f448986930dd4e823e957).\n",
        "  2. Unzip the data. Put it into your Google Drive, run the cells below to mount it to Colab:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c3dQChuccqfN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "12d824c3-c8d5-4d9c-f67a-0c8f33d7bb84"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "import os\n",
        "drive.mount('/content/drive', force_remount=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uFXI7NRHn1Cc"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "\n",
        "# TODO: please change the line below with your drive organization\n",
        "path = os.path.join(os.getcwd(), \"drive\", \"MyDrive\")\n",
        "\n",
        "with open(os.path.join(path,'train.json'), 'r') as f:\n",
        "     train = json.loads(f.read())\n",
        "\n",
        "with open(os.path.join(path,'val.json'), 'r') as f:\n",
        "     val = json.loads(f.read())\n",
        "\n",
        "with open(os.path.join(path,'test.json'), 'r') as f:\n",
        "     test = json.loads(f.read())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zfjKFeE_7T7C"
      },
      "source": [
        "Here's a few things to note about the dataset above:\n",
        "1. We have loaded 3 `.json` files: for training, validation, and testing.\n",
        "2. The train and validation files contain the following 3 fields (each is a nested list): \n",
        "  - **'text'** - actual input tokens\n",
        "  - **'NER'** - the token-level entity tag \n",
        "  - **'index'** - index of the token in the dataset\n",
        "3. The test data only has **'text'**, and **'index'** fields. You will need to submit your prediction of the **'NER'** tag to Kaggle."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cradDk-37G8L"
      },
      "source": [
        "\n",
        "### **Q1: Initial Data Observations**\n",
        "\n",
        "In the space below please add your code for dataset explorations for Q1."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PvvRSlhb6sAR"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "train_data = pd.DataFrame({'text':train['text'],'index':train['index'],'NER':train['NER']})\n",
        "test_data = pd.DataFrame({'text':test['text'],'index':test['index'],'NER':val['NER']})\n",
        "val_data = pd.DataFrame({'text':val['text'],'index':val['index'],'NER':val['NER']})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ijhVJVqhTTZ4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 333
        },
        "outputId": "31eb9fc5-9ded-475f-d3d8-e6864d3efba0"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                                text  \\\n",
              "0  [Because, of, its, high, rate, of, economic, m...   \n",
              "1  [Every, aspect, of, life, was, regulated, to, ...   \n",
              "2  [Yet, despite, his, impending, death, ,, Louis...   \n",
              "3  [Direct, fluorescent, antibody, can, also, be,...   \n",
              "4  [It, has, yet, to, be, released, on, CD, ,, th...   \n",
              "\n",
              "                                               index  \\\n",
              "0  [94033, 94034, 94035, 94036, 94037, 94038, 940...   \n",
              "1  [1805713, 1805714, 1805715, 1805716, 1805717, ...   \n",
              "2  [839361, 839362, 839363, 839364, 839365, 83936...   \n",
              "3  [1042749, 1042750, 1042751, 1042752, 1042753, ...   \n",
              "4  [464713, 464714, 464715, 464716, 464717, 46471...   \n",
              "\n",
              "                                                 NER  \n",
              "0  [O, O, O, O, O, O, O, O, O, O, O, O, O, B-LOC,...  \n",
              "1  [O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...  \n",
              "2           [O, O, O, O, O, O, B-PER, O, O, O, O, O]  \n",
              "3  [O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...  \n",
              "4  [O, O, O, O, O, O, O, O, O, O, O, B-ORG, O, O,...  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-8f3d908f-988f-488b-bb9d-ef1ebfa8051f\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>text</th>\n",
              "      <th>index</th>\n",
              "      <th>NER</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>[Because, of, its, high, rate, of, economic, m...</td>\n",
              "      <td>[94033, 94034, 94035, 94036, 94037, 94038, 940...</td>\n",
              "      <td>[O, O, O, O, O, O, O, O, O, O, O, O, O, B-LOC,...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>[Every, aspect, of, life, was, regulated, to, ...</td>\n",
              "      <td>[1805713, 1805714, 1805715, 1805716, 1805717, ...</td>\n",
              "      <td>[O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>[Yet, despite, his, impending, death, ,, Louis...</td>\n",
              "      <td>[839361, 839362, 839363, 839364, 839365, 83936...</td>\n",
              "      <td>[O, O, O, O, O, O, B-PER, O, O, O, O, O]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>[Direct, fluorescent, antibody, can, also, be,...</td>\n",
              "      <td>[1042749, 1042750, 1042751, 1042752, 1042753, ...</td>\n",
              "      <td>[O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>[It, has, yet, to, be, released, on, CD, ,, th...</td>\n",
              "      <td>[464713, 464714, 464715, 464716, 464717, 46471...</td>\n",
              "      <td>[O, O, O, O, O, O, O, O, O, O, O, B-ORG, O, O,...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-8f3d908f-988f-488b-bb9d-ef1ebfa8051f')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-8f3d908f-988f-488b-bb9d-ef1ebfa8051f button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-8f3d908f-988f-488b-bb9d-ef1ebfa8051f');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ],
      "source": [
        "train_data.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Me89sOLyNlSi",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 234
        },
        "outputId": "06327432-7737-40f4-82e8-2749da6bbc81"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                                     text  \\\n",
              "count                                                7000   \n",
              "unique                                               7000   \n",
              "top     [Because, of, its, high, rate, of, economic, m...   \n",
              "freq                                                    1   \n",
              "\n",
              "                                                    index  \\\n",
              "count                                                7000   \n",
              "unique                                               7000   \n",
              "top     [94033, 94034, 94035, 94036, 94037, 94038, 940...   \n",
              "freq                                                    1   \n",
              "\n",
              "                                                NER  \n",
              "count                                          7000  \n",
              "unique                                         5368  \n",
              "top     [O, O, O, O, O, O, O, O, O, O, O, B-LOC, O]  \n",
              "freq                                             18  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-785268d2-0f5a-445b-995b-673b4d018295\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>text</th>\n",
              "      <th>index</th>\n",
              "      <th>NER</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>count</th>\n",
              "      <td>7000</td>\n",
              "      <td>7000</td>\n",
              "      <td>7000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>unique</th>\n",
              "      <td>7000</td>\n",
              "      <td>7000</td>\n",
              "      <td>5368</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>top</th>\n",
              "      <td>[Because, of, its, high, rate, of, economic, m...</td>\n",
              "      <td>[94033, 94034, 94035, 94036, 94037, 94038, 940...</td>\n",
              "      <td>[O, O, O, O, O, O, O, O, O, O, O, B-LOC, O]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>freq</th>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>18</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-785268d2-0f5a-445b-995b-673b4d018295')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-785268d2-0f5a-445b-995b-673b4d018295 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-785268d2-0f5a-445b-995b-673b4d018295');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ],
      "source": [
        "train_data.describe()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MaACoFkJRTy-",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 234
        },
        "outputId": "4bfca597-5484-4a7f-eaf5-83546d7e246f"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                                     text  \\\n",
              "count                                                 400   \n",
              "unique                                                400   \n",
              "top     [They, had, met, during, a, softball, game, fo...   \n",
              "freq                                                    1   \n",
              "\n",
              "                                                    index  \\\n",
              "count                                                 400   \n",
              "unique                                                400   \n",
              "top     [57004, 57005, 57006, 57007, 57008, 57009, 570...   \n",
              "freq                                                    1   \n",
              "\n",
              "                                     NER  \n",
              "count                                400  \n",
              "unique                               390  \n",
              "top     [O, O, O, O, O, B-PER, I-PER, O]  \n",
              "freq                                   4  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-a23f3f50-c78f-4847-a7c9-9278cbc610da\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>text</th>\n",
              "      <th>index</th>\n",
              "      <th>NER</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>count</th>\n",
              "      <td>400</td>\n",
              "      <td>400</td>\n",
              "      <td>400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>unique</th>\n",
              "      <td>400</td>\n",
              "      <td>400</td>\n",
              "      <td>390</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>top</th>\n",
              "      <td>[They, had, met, during, a, softball, game, fo...</td>\n",
              "      <td>[57004, 57005, 57006, 57007, 57008, 57009, 570...</td>\n",
              "      <td>[O, O, O, O, O, B-PER, I-PER, O]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>freq</th>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-a23f3f50-c78f-4847-a7c9-9278cbc610da')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-a23f3f50-c78f-4847-a7c9-9278cbc610da button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-a23f3f50-c78f-4847-a7c9-9278cbc610da');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ],
      "source": [
        "val_data.describe()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PEyNgeAvRj6v",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 234
        },
        "outputId": "00b7e6e9-0953-4d13-e450-9a573fc5b49b"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                                     text  \\\n",
              "count                                                 400   \n",
              "unique                                                400   \n",
              "top     [The, Yankees, won, the, series, in, five, gam...   \n",
              "freq                                                    1   \n",
              "\n",
              "                                                    index  \\\n",
              "count                                                 400   \n",
              "unique                                                400   \n",
              "top     [146976, 146977, 146978, 146979, 146980, 14698...   \n",
              "freq                                                    1   \n",
              "\n",
              "                                     NER  \n",
              "count                                400  \n",
              "unique                               390  \n",
              "top     [O, O, O, O, O, B-PER, I-PER, O]  \n",
              "freq                                   4  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-f4f56c95-2257-4138-a58c-9a08e0c89bcd\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>text</th>\n",
              "      <th>index</th>\n",
              "      <th>NER</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>count</th>\n",
              "      <td>400</td>\n",
              "      <td>400</td>\n",
              "      <td>400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>unique</th>\n",
              "      <td>400</td>\n",
              "      <td>400</td>\n",
              "      <td>390</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>top</th>\n",
              "      <td>[The, Yankees, won, the, series, in, five, gam...</td>\n",
              "      <td>[146976, 146977, 146978, 146979, 146980, 14698...</td>\n",
              "      <td>[O, O, O, O, O, B-PER, I-PER, O]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>freq</th>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-f4f56c95-2257-4138-a58c-9a08e0c89bcd')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-f4f56c95-2257-4138-a58c-9a08e0c89bcd button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-f4f56c95-2257-4138-a58c-9a08e0c89bcd');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ],
      "source": [
        "test_data.describe()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YAHGHEgQSFoG",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 313
        },
        "outputId": "ed28a072-5625-4128-da8d-106b8645eb21"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "the longest document length is  131\n",
            "the shortest document length is  2\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "# document lengths\n",
        "import numpy as np\n",
        "document= np.asarray(train_data['text'])\n",
        "\n",
        "la= []\n",
        "for i in range(document.size):\n",
        "  la.append(np.asarray(document[i]).size)\n",
        "import matplotlib.pyplot as plt\n",
        "print(\"the longest document length is \",max(la))\n",
        "print(\"the shortest document length is \",min(la))\n",
        "plt.hist(la, bins= 40)\n",
        "plt.xlabel(\"document length\")\n",
        "plt.ylabel(\"count\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3tjcKa4vSLeU"
      },
      "outputs": [],
      "source": [
        "token_class = {}\n",
        "entity_class = {}\n",
        "for label in train_data['NER']:\n",
        "  for l in label:\n",
        "    token_class[l] = token_class.get(l,0)+1\n",
        "    if len(l)>1:\n",
        "      ls = l.split('-')[1]\n",
        "      entity_class[ls] = entity_class.get(ls,0)+1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ADUwUnAbSLui",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 418
        },
        "outputId": "e9abeb6e-2bd9-496e-e578-8fbc0f38bd49"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'O': 147113,\n",
              " 'B-LOC': 4060,\n",
              " 'B-PER': 3016,\n",
              " 'I-PER': 2217,\n",
              " 'B-ORG': 1725,\n",
              " 'B-MISC': 2815,\n",
              " 'I-LOC': 1451,\n",
              " 'I-MISC': 2566,\n",
              " 'I-ORG': 1431}"
            ]
          },
          "metadata": {},
          "execution_count": 10
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "plt.bar(list(token_class.keys())[1:], list(token_class.values())[1:])\n",
        "token_class"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6PwHlWsxSMI6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 282
        },
        "outputId": "d04fa4ff-0441-489f-ba09-ae2265cd4e04"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<BarContainer object of 4 artists>"
            ]
          },
          "metadata": {},
          "execution_count": 11
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "plt.bar(entity_class.keys(), entity_class.values())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rfoShC--SMUq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c591ba85-7216-4fc9-cdfa-e6f953a3898c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Because of its high rate of economic marginalization , more people migrate from [B-LOC Chiapas] than migrate to it . \n",
            "Every aspect of life was regulated to some degree by the party , and the will of its founding-president , [B-PER Mobutu] [I-PER Sese] [I-PER Seko] . \n",
            "Yet despite his impending death , [B-PER Louis] 's mind remained clear . \n"
          ]
        }
      ],
      "source": [
        "for i in range(3):\n",
        "  s= \"\"\n",
        "  s1= train_data['text'][i]\n",
        "  s2= train_data['NER'][i]\n",
        "  for idx, word in enumerate(s1):\n",
        "    if(len(s2[idx])>1):\n",
        "      s+= '['+s2[idx]+' '+word+']'+' '\n",
        "    else:\n",
        "      s+=word+' '\n",
        "  print(s)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VyfRAMbaSF1_"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sO7go9ivDusU"
      },
      "source": [
        "<a name=\"q1\"></a>\n",
        "[[^^^]](#outline) \n",
        "\n",
        "What are your initial observations after you explore the dataset?  Provide some quantitative data exploration. Assess dataset size, document lengths and the token-level NER class distribution, and the entity-level NER class distribution (skipping the 'O' label for the latter). Give some examples of sentences with their named entities bracketed, e.g. [[B-LOC Romania] state budget soars in June .] and [[B-ORG Zifa] said [B-PER Renate] [I-PER Goetschl] of [B-LOC Austria]...]. \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dVbqyFYq1tOy"
      },
      "source": [
        "#### **A1:**\n",
        "\n",
        "As we explored the dataset, we made some observations:\n",
        "\n",
        "1. The train dataset has 7000 entries, test dataset has 400 entries and validation set has 400 entries\n",
        "2. The longest document length is 131, versus the shortest document length is 2, and the most common lengths are located between 10 - 20.\n",
        "3. The token-level NER class distribution, and the entity-level NER class distribution has shown above via bar chart, they both distributed well\n",
        "4. Eg words: \n",
        "  - Because of its high rate of economic marginalization , more people migrate from [B-LOC Chiapas] than migrate to it . \n",
        "  - Every aspect of life was regulated to some degree by the party , and the will of its founding-president , [B-PER Mobutu] [I-PER Sese] [I-PER Seko] . \n",
        "  - Yet despite his impending death , [B-PER Louis] 's mind remained clear . "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4NYvfchqqBf6"
      },
      "source": [
        "<a name=\"part2\"></a>\n",
        "# **Part 2: Hidden Markov Model**\n",
        "[[^^^]](#outline) \n",
        "---\n",
        "In this part of the assignment, you will:\n",
        "1. Implement code for counting and smoothing of labels and words, as well as unkown word handing, as necessary to support the Viterbi algorithm. \n",
        "2. Build a Hidden Markov Model in accordance with the provided function headers. **You may NOT change the function specifications.** Please ensure that your code is clear, concise, and, most important of all, modular. This means you should break your implementation down into smaller functions or write it within a class. Please compute all probabilities in natural log when building the HMM.\n",
        "3. Implement the **Viterbi algorithm**, that can be used to infer token-level labels (identifying the appropriate named entity) for an input document. This process is commonly referred to as **decoding**. Bigram-based Viterbi is $ \\mathcal{O}(sm^2)$ where *s* is the length of the sentence and *m* is the number of tags. Your implementation should have similar efficiency. The code for this can be used later on for the MEMM too.\n",
        "\n",
        "### References\n",
        "You may find chapters [3](https://web.stanford.edu/~jurafsky/slp3/3.pdf) and [8](https://web.stanford.edu/~jurafsky/slp3/8.pdf) of Jurafsky and Martin book useful. In particular, section 3.4.1 covers ways to handle unknown words, and section 3.5 goes over smoothing. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-jUVJwSaE1tI"
      },
      "source": [
        "<a name=\"unknowns_handling\"></a>\n",
        "## **Unknown Word Handling**\n",
        "[[^^^]](#outline) \n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "44Bja4eQEMJR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3cf7a6b3-3568-43fa-880a-4394a89bb730"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The number that set of unknown word is 889\n",
            "the size of Vocabulary: 24823\n"
          ]
        }
      ],
      "source": [
        "# Implement unknown word handling here! You may do this any way that you please\n",
        "\n",
        "# handling unkown words\n",
        "\n",
        "words= {}\n",
        "for s in train_data['text']:\n",
        "  s1= s\n",
        "  for idx, word in enumerate(s1):\n",
        "    words[word]= words.get(word, 0) +1\n",
        "\n",
        "threshold= 1\n",
        "\n",
        "token_new= []\n",
        "r=0\n",
        "for s in train_data['text']:\n",
        "  l2=[]\n",
        "  for i in s:\n",
        "    if words[i] <= threshold and len(i) > 12:\n",
        "      r+=1\n",
        "      l2.append(\"<UNK>\")\n",
        "    else:\n",
        "      l2.append(i)  \n",
        "  token_new.append(l2)  \n",
        "\n",
        "\n",
        "print(\"The number that set of unknown word is\", r)\n",
        "print(\"the size of Vocabulary:\",len(words))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VHH1Sd2_FkSM"
      },
      "outputs": [],
      "source": [
        "words_new= {}\n",
        "for i in range(len(token_new)):\n",
        "  for j in range(len(token_new[i])):\n",
        "    if(token_new[i][j] == \"<UNK>\"):\n",
        "      words_new[train_data['NER'][i][j]]= words_new.get(train_data['NER'][i][j], 0) +1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aLuTFUV5FA3m"
      },
      "source": [
        "<a name=\"hmm_implementation\"></a>\n",
        "## **HMM Implementation**\n",
        "[[^^^]](#outline) \n",
        "---\n",
        "\n",
        "In the skeleton code below, we have broken down the HMM into its three components: the transition matrix, the emission matrix, and the start state probabilities. We suggest you implement them separately and then use them to build the HMM.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NBJgSrRtFZuG"
      },
      "outputs": [],
      "source": [
        "import math\n",
        "# Returns the transition probabilities.\n",
        "def build_transition_matrix(labels, k=0):\n",
        "  \"\"\"\n",
        "    Returns a dictionary that has tuples of every label bigram as keys, and\n",
        "    the associated value being the respective transition probabilities (in \n",
        "    natural log).\n",
        "    Eg. {(\"O\", \"B-ORG\"): -9.98690147425591, \n",
        "         (\"B-LOC\", \"I-LOC\"): -3.69537214,\n",
        "         ...,\n",
        "         ...,\n",
        "        }\n",
        "    \n",
        "    :parameter labels: A list where each element represents a sentence, \n",
        "    and each sentence is a list of NER labels for each of its tokens. (Eg. \n",
        "    [['O', 'O', 'O', 'O', 'O', 'O', 'B-LOC', 'O'], [...], ...])\n",
        "    :type labels: List[List[String]]\n",
        "    :parameter k: an optional parameter for smoothing\n",
        "    :type k: int\n",
        "    \"\"\"\n",
        "  labels_new=[]\n",
        "  for i in labels:\n",
        "    l=[]\n",
        "    for j in i:\n",
        "      l.append(j)\n",
        "    l.append(\"<end>\")\n",
        "    labels_new.append(l)  \n",
        "\n",
        "  label_map= {}\n",
        "  for la in labels_new:\n",
        "    for l in la:\n",
        "      label_map[l]= label_map.get(l, 0)+1\n",
        "  label_list= list(label_map.keys())\n",
        "\n",
        "  res={}\n",
        "\n",
        "  for l in label_list:\n",
        "    for t in label_list:\n",
        "      res[(l,t)]= res.get((l,t), 0) +k # smoothing\n",
        "\n",
        "  for i in range(len(labels_new)):\n",
        "    for j in range(len(labels_new[i])):\n",
        "      if j>0 :\n",
        "        l= labels_new[i][j-1]\n",
        "        t= labels_new[i][j]\n",
        "        res[(l,t)]= res.get((l,t))+1\n",
        "  for l in label_list:\n",
        "    for t in label_list:\n",
        "      res[(l,t)]= math.log10(res.get((l,t))/ (label_map.get(l) +(len(label_list)* k)))\n",
        "  return res\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gRt-pjh4FvZ2"
      },
      "outputs": [],
      "source": [
        "# Returns the emission probabilities.\n",
        "import math\n",
        "def build_emission_matrix(tokens, labels, k=0):\n",
        "  \"\"\"\n",
        "    Returns a dictionary that has label-token tuples as keys, and emission \n",
        "    probabilities (in natural log) for each respective label-token pair as values.  \n",
        "\n",
        "    Eg. {(\"O\", \"Because\"): -10.133904545421267, \n",
        "         (\"I-PER\", \"Markov\"): -7.428569227340841,\n",
        "         ...,\n",
        "         ...,\n",
        "        }\n",
        "    \n",
        "    :parameter tokens: A list where each element represents a sentence, \n",
        "    and each sentence is a list of its tokens. (Eg. [['The', 'most', \n",
        "    'significant', 'damage', 'was', 'on', 'Tortola', '.'], [...], ...])\n",
        "    :type tokens: List[List[String]]\n",
        "    :parameter labels: A list where each element represents a sentence, \n",
        "    and each sentence is a list of NER labels for each of its tokens. (Eg. \n",
        "    [['O', 'O', 'O', 'O', 'O', 'O', 'B-LOC', 'O'], [...], ...])\n",
        "    :type labels: List[List[String]]\n",
        "    :parameter k: an optional parameter for smoothing\n",
        "    :type k: int\n",
        "    \"\"\"\n",
        "  token_map= {}\n",
        "  label_map= {}\n",
        "  for la in labels:\n",
        "    for l in la:\n",
        "      label_map[l]= label_map.get(l, 0)+1\n",
        "  for token in tokens:\n",
        "    for t in token:\n",
        "      token_map[t]= token_map.get(t, 0)+1\n",
        "  label_list= list(label_map.keys())\n",
        "  token_list= list(token_map.keys())\n",
        "\n",
        "  res={}\n",
        "  for l in label_list:\n",
        "    for t in token_list:\n",
        "      res[(l,t)]= res.get((l,t), 0) +k # smoothing\n",
        "\n",
        "  for i in range(len(labels)):\n",
        "    for j in range(len(labels[i])):\n",
        "      l= labels[i][j]\n",
        "      t= tokens[i][j]\n",
        "      res[(l,t)]= res.get((l,t))+1\n",
        "\n",
        "  for l in label_list:\n",
        "    for t in token_list:\n",
        "      res[(l,t)]= math.log10(res.get((l,t))/ ((len(token_list) * k) + label_map.get(l)))\n",
        "  return res\n",
        "  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VTN9B6k0HK77"
      },
      "outputs": [],
      "source": [
        "# Returns the starting state probabilities.\n",
        "def get_start_state_probs(labels, k=0):\n",
        "  \"\"\"\n",
        "    Returns a dictionary that has labels for keys, and the respective state \n",
        "    probabilities (in natural log) for values.  \n",
        "\n",
        "    Eg. {\"O\": -10.133904545421267, \n",
        "         \"I-PER\": -7.428569227340841,\n",
        "         ...,\n",
        "         ...,\n",
        "        }\n",
        "    \n",
        "    :parameter labels: A list where each element represents a sentence, \n",
        "    and each sentence is a list of NER labels for each of its tokens. (Eg. \n",
        "    [['O', 'O', 'O', 'O', 'O', 'O', 'B-LOC', 'O'], [...], ...])\n",
        "    :type labels: List[List[String]]\n",
        "    :parameter k: an optional parameter for smoothing\n",
        "    :type k: int\n",
        "    \"\"\"\n",
        "  label_map= {}\n",
        "  for la in labels:\n",
        "    for l in la:\n",
        "      label_map[l]= label_map.get(l, 0)+1\n",
        "  label_list= list(label_map.keys())\n",
        "\n",
        "  res= {}\n",
        "  for l in label_list:\n",
        "    res[l]= res.get(l, 0)+k\n",
        "  for i in range(len(labels)):\n",
        "    res[labels[i][0]]= res.get(labels[i][0],0)+1\n",
        "\n",
        "  for l in label_list:\n",
        "    res[l]= math.log10(res.get(l)/ (len(labels)+ (k*len(label_list))))\n",
        "\n",
        "  return res"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "get_start_state_probs(train['NER'], k=0.1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zBnb-AfiP56H",
        "outputId": "d8c8f6c9-7a98-4bf3-de0d-13d39c1611b7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'O': -0.038831420800450184,\n",
              " 'B-LOC': -1.6221774243935554,\n",
              " 'B-PER': -1.4025173485047149,\n",
              " 'I-PER': -4.845153874286947,\n",
              " 'B-ORG': -1.9812364973290861,\n",
              " 'B-MISC': -1.9361330200757907,\n",
              " 'I-LOC': -4.845153874286947,\n",
              " 'I-MISC': -4.845153874286947,\n",
              " 'I-ORG': -4.845153874286947}"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LU3Ff-CKInqo"
      },
      "outputs": [],
      "source": [
        "# Takes in the tokens & labels and returns a representation of the HMM.\n",
        "# Call the three functions above in this function to build your HMM.\n",
        "def build_hmm(tokens, labels):\n",
        "  res = {}\n",
        "  res[\"transition\"] = build_transition_matrix(labels, k=1)\n",
        "  res[\"emission\"] = build_emission_matrix(tokens, labels, k=0.01)\n",
        "  res[\"start\"] = get_start_state_probs(labels, k=1)\n",
        "  label_map= {}\n",
        "  for la in labels:\n",
        "    for l in la:\n",
        "      label_map[l]= label_map.get(l, 0)+1\n",
        "  res[\"labels\"] = list(label_map.keys())\n",
        "  token_map= {}\n",
        "  for to in tokens:\n",
        "    for t in to:\n",
        "      token_map[t]= token_map.get(t, 0) +1\n",
        "  res[\"tokens\"]= list(token_map.keys())\n",
        "  return res"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pbw3RTHPI31j"
      },
      "source": [
        "<a name=\"viterbi_implementation\"></a>\n",
        "## **Viterbi Implementation**\n",
        "[[^^^]](#outline) \n",
        "---\n",
        "\n",
        "At the end of your implementation, we expect a function or class that maps a sequence of tokens (observation) to a sequence of labels via the Viterbi algorithm."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C_q3U42lI3LQ"
      },
      "outputs": [],
      "source": [
        "import sys\n",
        "# Takes in the HMM built above and an observation (i.e. a list of tokens),\n",
        "# and returns a list with predicted named entity mappings for the tokens.\n",
        "# The returned list should be the same length as the input obeservation.\n",
        "def viterbi(hmm, observation):\n",
        "  start_probs = hmm[\"start\"]\n",
        "  emission_probs = hmm[\"emission\"]\n",
        "  transition_probs = hmm[\"transition\"]\n",
        "  labels_list = hmm[\"labels\"]\n",
        "  tokens_list= hmm[\"tokens\"]\n",
        "  token_set= set(tokens_list)\n",
        "  for i in range(len(observation)):\n",
        "    if observation[i] not in token_set:\n",
        "      observation[i]= \"<UNK>\"\n",
        "\n",
        "  backpointer= [[0]*len(observation) for i in range(len(labels_list))] # [label][obser]\n",
        "  dp= [[0]*len(observation) for i in range(len(labels_list))]\n",
        "  for i in range(len(labels_list)):\n",
        "    dp[i][0]= start_probs[labels_list[i]] + emission_probs[(labels_list[i], observation[0])]\n",
        "  \n",
        "  for i in range(1, len(observation)):\n",
        "    pointer=-sys.maxsize - 1\n",
        "    for j in range(len(labels_list)):\n",
        "      m= -sys.maxsize - 1\n",
        "      for k in range(len(labels_list)):\n",
        "        cur= dp[k][i-1] + transition_probs[(labels_list[k], labels_list[j])] + emission_probs[(labels_list[j], observation[i])] \n",
        "        if cur > m:\n",
        "          m= cur\n",
        "          backpointer[j][i]= k\n",
        "      dp[j][i]= m  \n",
        "  m= -sys.maxsize - 1\n",
        "  idx= -1\n",
        "  for i in range(len(labels_list)):\n",
        "    cur= dp[i][len(observation)-1] + transition_probs[(labels_list[i], '<end>')]\n",
        "    if cur > m:\n",
        "      m= cur\n",
        "      idx= i\n",
        "  res=[]\n",
        "  res.append(labels_list[idx])\n",
        "  for j in range(len(observation)-1, 0, -1):\n",
        "    #print(backpointer[idx][j])\n",
        "    idx= backpointer[idx][j]\n",
        "    res.append(labels_list[idx])\n",
        "  res.reverse()\n",
        "  return res\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ie6NXgJAKOEM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "02ab6212-2399-4a53-89b0-037055288120"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['O', 'O', 'O', 'B-ORG', 'I-ORG', 'O']"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ],
      "source": [
        "# Here's a sample observation that you can use to test your code\n",
        "#obs_1 =[\"Mobutu\", \"Sese\", \"Seko\", \".\",\"Yet\", \"despite\", \"his\", \"impending\", \"death\" , \",\", \"Louis\",\"'s\", \"mind\", \"remained\", \"clear\"] \n",
        "# Yet despite his impending death , [B-PER Louis] 's mind remained clear . \n",
        "#[B-PER Mobutu] [I-PER Sese] [I-PER Seko] .  Yet despite his impending death , [B-PER Louis] 's mind remained clear . \n",
        "obs_1 = ['A',\n",
        " 'spokesman',\n",
        " 'for',\n",
        " 'Cornell',\n",
        " 'University',\n",
        " 'said']\n",
        "\n",
        "# Uncomment and fill out the following line to test your implementation:\n",
        "hmm = build_hmm(token_new, train_data['NER'])\n",
        "a=viterbi(build_hmm(token_new, train_data['NER']),obs_1)\n",
        "a"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2J8NdM_V_A4u"
      },
      "source": [
        "## **Validation Step**\n",
        "<a name=\"validation_data\"></a>\n",
        "[[^^^]](#outline) \n",
        "---\n",
        "\n",
        "In this part of the project, we expect you to train your HMM model (i.e., get transition and emission probabilities) on the labeled training data and evaluate it on the validation data. Report **Entity Level Mean F1**, which was explained earlier. Please use the code we have provided below to compute this metric.\n",
        "\n",
        "Please also take a look into your misclassified cases, as we will be performing error analysis in the *Evaluation* section. We expect smoothing, unknown word handling and correct emission (i.e., lexical generation) probabilities."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oTAhu_TG1V0R"
      },
      "source": [
        "Consider the example below. After getting a sequence of NER labels for the sequence of tokens from your Viterbi algorithm implementation, you need to convert the sequence of tokens, associated token indices and NER labels into a format which can be used to calculate **Entity Level Mean F1**. We do this by finding the starting and ending indices of the spans representing each entity (as given in the corpus) and adding it to a list that is associated with the label with which the spans are labelled. To score your validation data on Google Colab or your local device, you can get a dictionary from the function **format_output_labels** on both the predicted and true label sequences, and use the two dictionaries as input to the **mean_f1** function.\n",
        "\n",
        "NOTE: We do **not** include the spans of the tokens labelled as \"O\" in the formatted dictionary output."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HdOOQdN7D2rv"
      },
      "outputs": [],
      "source": [
        "def format_output_labels(token_labels, token_indices):\n",
        "    \"\"\"\n",
        "    Returns a dictionary that has the labels (LOC, ORG, MISC or PER) as the keys, \n",
        "    with the associated value being the list of entities predicted to be of that key label. \n",
        "    Each entity is specified by its starting and ending position indicated in [token_indices].\n",
        "\n",
        "    Eg. if [token_labels] = [\"B-ORG\", \"I-ORG\", \"O\", \"O\", \"B-ORG\"]\n",
        "           [token_indices] = [15, 16, 17, 18, 19]\n",
        "        then dictionary returned is \n",
        "        {'LOC': [], 'MISC': [], 'ORG': [(15, 16), (19, 19)], 'PER': []}\n",
        "\n",
        "    :parameter token_labels: A list of token labels (eg. B-PER, I-PER, B-LOC, I-LOC, B-ORG, I-ORG, B-MISC, OR I-MISC).\n",
        "    :type token_labels: List[String]\n",
        "    :parameter token_indices: A list of token indices (taken from the dataset) \n",
        "                              corresponding to the labels in [token_labels].\n",
        "    :type token_indices: List[int]\n",
        "    \"\"\"\n",
        "    label_dict = {\"LOC\":[], \"MISC\":[], \"ORG\":[], \"PER\":[]}\n",
        "    prev_label = 'O'\n",
        "    entity_label = ''\n",
        "    start_idx = 0\n",
        "    ll = len(token_labels)\n",
        "    for idx, label in enumerate(token_labels):\n",
        "      curr_label = label.split('-')[-1]\n",
        "      if label.startswith(\"B\"):\n",
        "        start_idx = token_indices[idx]\n",
        "        entity_label = curr_label\n",
        "      if label == 'O' and prev_label != \"O\":\n",
        "        label_dict[entity_label].append((start_idx, token_indices[idx-1]))\n",
        "      if label.startswith('I') and prev_label == 'O':\n",
        "        start_idx = token_indices[idx]\n",
        "        entity_label = curr_label\n",
        "      if (label.startswith('B') or label.startswith('I')) and idx+1 == ll:\n",
        "        label_dict[entity_label].append((start_idx, token_indices[idx]))\n",
        "      prev_label = label[0]\n",
        "    return label_dict\n",
        "\n",
        "    # for idx, label in enumerate(token_labels):\n",
        "    #   curr_label = label.split('-')[-1]\n",
        "    #   if label.startswith(\"B-\") or (curr_label != prev_label and curr_label != \"O\"):\n",
        "    #     if prev_label != \"O\":\n",
        "    #         label_dict[prev_label].append((start, token_indices[idx-1]))\n",
        "    #     start = token_indices[idx]\n",
        "    #   elif label == \"O\" and prev_label != \"O\":\n",
        "    #     label_dict[prev_label].append((start, token_indices[idx-1]))\n",
        "    #     start = None\n",
        "\n",
        "    #   prev_label = curr_label\n",
        "    # if start is not None:\n",
        "    #   label_dict[prev_label].append((start, token_indices[idx-1]))\n",
        "    # return label_dict"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lfjVJLNhL_fc"
      },
      "outputs": [],
      "source": [
        "# Code for mean F1\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "def mean_f1(y_pred_dict, y_true_dict):\n",
        "    \"\"\" \n",
        "    Calculates the entity-level mean F1 score given the actual/true and \n",
        "    predicted span labels.\n",
        "    :parameter y_pred_dict: A dictionary containing predicted labels as keys and the \n",
        "                            list of associated span labels as the corresponding\n",
        "                            values.\n",
        "    :type y_pred_dict: Dict<key [String] : value List[Tuple]>\n",
        "    :parameter y_true_dict: A dictionary containing true labels as keys and the \n",
        "                            list of associated span labels as the corresponding\n",
        "                            values.\n",
        "    :type y_true_dict: Dict<key [String] : value List[Tuple]>\n",
        "\n",
        "    Implementation modified from original by author @shonenkov at\n",
        "    https://www.kaggle.com/shonenkov/competition-metrics.\n",
        "    \"\"\"\n",
        "    F1_lst = []\n",
        "    for key in y_true_dict:\n",
        "        TP, FN, FP = 0, 0, 0\n",
        "        num_correct, num_true = 0, 0\n",
        "        preds = y_pred_dict[key]\n",
        "        trues = y_true_dict[key]\n",
        "        for true in trues:\n",
        "            num_true += 1\n",
        "            if true in preds:\n",
        "                num_correct += 1\n",
        "            else:\n",
        "                continue\n",
        "        num_pred = len(preds)\n",
        "        if num_true != 0:\n",
        "            if num_pred != 0 and num_correct != 0:\n",
        "                R = num_correct / num_true\n",
        "                P = num_correct / num_pred\n",
        "                F1 = 2*P*R / (P + R)\n",
        "            else:\n",
        "                F1 = 0      # either no predictions or no correct predictions\n",
        "        else:\n",
        "            continue\n",
        "        F1_lst.append(F1)\n",
        "    return np.mean(F1_lst)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5rwLFPhetVok",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a13fc824-7ff7-4b54-cfb2-70205aedc6d7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "y_pred_dict is : {'LOC': [(18, 18), (28, 28)], 'MISC': [(23, 23)], 'ORG': [(13, 13)], 'PER': [(15, 16)]}\n",
            "y_true_dict is : {'LOC': [(18, 18), (28, 28)], 'MISC': [(23, 24)], 'ORG': [(13, 13)], 'PER': [(15, 16)]}\n",
            "Entity Level Mean F1 score is : 0.75\n"
          ]
        }
      ],
      "source": [
        "# Usage using above example\n",
        "#'<UNK>', 'University', 'is', 'located', 'in', 'Ithaca', 'and', 'was', 'founded', 'by', 'Ezra', 'Cornell'\n",
        "pred_token_labels = [\"B-ORG\", \"O\", \"B-PER\", \"I-PER\", \"O\", \"B-LOC\", \"O\", \"O\", \"O\", \"O\", \"B-MISC\", \"O\", \"O\", \"O\", \"O\", \"B-LOC\"]\n",
        "true_token_labels = [\"B-ORG\", \"O\", \"B-PER\", \"I-PER\", \"O\", \"B-LOC\", \"O\", \"O\", \"O\", \"O\", \"B-MISC\", \"I-MISC\", \"O\", \"O\", \"O\", \"B-LOC\"]\n",
        "token_indices = [13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28]\n",
        "\n",
        "y_pred_dict = format_output_labels(pred_token_labels, token_indices)\n",
        "print(\"y_pred_dict is : \" + str(y_pred_dict))\n",
        "y_true_dict = format_output_labels(true_token_labels, token_indices)\n",
        "print(\"y_true_dict is : \" + str(y_true_dict))\n",
        "\n",
        "print(\"Entity Level Mean F1 score is : \" + str(mean_f1(y_pred_dict, y_true_dict)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0Lmx9z5WGAfq"
      },
      "outputs": [],
      "source": [
        "# Evaluate/validate your model here\n",
        "pred_token_labels_ll = []\n",
        "true_token_labels_ll = []\n",
        "token_indices = []\n",
        "for i in range(len(val_data['text'])):\n",
        "  pred_token_labels = viterbi(hmm, val_data['text'][i])\n",
        "  pred_token_labels_ll.append(pred_token_labels)\n",
        "  true_token_labels_ll.append(val_data['NER'][i])\n",
        "  token_indices.append(val_data['index'][i])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hwHe2oS4mDVS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a3030739-ca26-4fb4-be8d-bfdf9b7c5b54"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0\n",
            "Entity Level Mean F1 score is : 1.0\n",
            "1\n",
            "Entity Level Mean F1 score is : 0.0\n",
            "2\n",
            "Entity Level Mean F1 score is : 0.0\n",
            "3\n",
            "Entity Level Mean F1 score is : 0.5\n",
            "4\n",
            "Entity Level Mean F1 score is : 0.0\n",
            "5\n",
            "Entity Level Mean F1 score is : 1.0\n",
            "6\n",
            "Entity Level Mean F1 score is : 0.0\n",
            "7\n",
            "Entity Level Mean F1 score is : 0.0\n",
            "8\n",
            "Entity Level Mean F1 score is : 0.0\n",
            "9\n",
            "Entity Level Mean F1 score is : 1.0\n",
            "10\n",
            "Entity Level Mean F1 score is : 1.0\n",
            "11\n",
            "Entity Level Mean F1 score is : 0.5\n",
            "12\n",
            "Entity Level Mean F1 score is : 1.0\n",
            "13\n",
            "Entity Level Mean F1 score is : 0.0\n",
            "14\n",
            "Entity Level Mean F1 score is : 0.0\n",
            "15\n",
            "Entity Level Mean F1 score is : 1.0\n",
            "16\n",
            "Entity Level Mean F1 score is : 0.0\n",
            "17\n",
            "Entity Level Mean F1 score is : 0.5\n",
            "18\n",
            "Entity Level Mean F1 score is : 1.0\n",
            "19\n",
            "Entity Level Mean F1 score is : 0.3333333333333333\n",
            "20\n",
            "Entity Level Mean F1 score is : 1.0\n",
            "21\n",
            "Entity Level Mean F1 score is : 1.0\n",
            "22\n",
            "Entity Level Mean F1 score is : 0.0\n",
            "23\n",
            "Entity Level Mean F1 score is : 0.0\n",
            "24\n",
            "label starts with I! Impossible: from indices 172688\n",
            "Entity Level Mean F1 score is : 0.0\n",
            "25\n",
            "Entity Level Mean F1 score is : 0.0\n",
            "26\n",
            "Entity Level Mean F1 score is : 0.0\n",
            "27\n",
            "Entity Level Mean F1 score is : 1.0\n",
            "28\n",
            "Entity Level Mean F1 score is : 0.0\n",
            "29\n",
            "Entity Level Mean F1 score is : 0.0\n",
            "30\n",
            "Entity Level Mean F1 score is : 0.0\n",
            "31\n",
            "Entity Level Mean F1 score is : 0.25\n",
            "32\n",
            "Entity Level Mean F1 score is : 0.0\n",
            "33\n",
            "Entity Level Mean F1 score is : 1.0\n",
            "34\n",
            "Entity Level Mean F1 score is : 1.0\n",
            "35\n",
            "Entity Level Mean F1 score is : 0.0\n",
            "36\n",
            "Entity Level Mean F1 score is : 1.0\n",
            "37\n",
            "Entity Level Mean F1 score is : 1.0\n",
            "38\n",
            "Entity Level Mean F1 score is : 0.0\n",
            "39\n",
            "Entity Level Mean F1 score is : 0.0\n",
            "40\n",
            "Entity Level Mean F1 score is : 1.0\n",
            "41\n",
            "Entity Level Mean F1 score is : 1.0\n",
            "42\n",
            "Entity Level Mean F1 score is : 0.0\n",
            "43\n",
            "label starts with I! Impossible: from indices 74133\n",
            "Entity Level Mean F1 score is : 1.0\n",
            "44\n",
            "Entity Level Mean F1 score is : 0.4\n",
            "45\n",
            "Entity Level Mean F1 score is : 0.0\n",
            "46\n",
            "Entity Level Mean F1 score is : 0.5\n",
            "47\n",
            "Entity Level Mean F1 score is : 0.8\n",
            "48\n",
            "Entity Level Mean F1 score is : 1.0\n",
            "49\n",
            "Entity Level Mean F1 score is : 1.0\n",
            "50\n",
            "Entity Level Mean F1 score is : 0.0\n",
            "51\n",
            "Entity Level Mean F1 score is : 1.0\n",
            "52\n",
            "Entity Level Mean F1 score is : 1.0\n",
            "53\n",
            "Entity Level Mean F1 score is : 0.0\n",
            "54\n",
            "Entity Level Mean F1 score is : 0.0\n",
            "55\n",
            "label starts with I! Impossible: from indices 246732\n",
            "Entity Level Mean F1 score is : 0.0\n",
            "56\n",
            "Entity Level Mean F1 score is : 1.0\n",
            "57\n",
            "Entity Level Mean F1 score is : 0.5\n",
            "58\n",
            "Entity Level Mean F1 score is : 0.5\n",
            "59\n",
            "Entity Level Mean F1 score is : 0.0\n",
            "60\n",
            "Entity Level Mean F1 score is : 0.5\n",
            "61\n",
            "Entity Level Mean F1 score is : 0.0\n",
            "62\n",
            "Entity Level Mean F1 score is : 0.0\n",
            "63\n",
            "Entity Level Mean F1 score is : 0.0\n",
            "64\n",
            "Entity Level Mean F1 score is : 1.0\n",
            "65\n",
            "Entity Level Mean F1 score is : 0.0\n",
            "66\n",
            "Entity Level Mean F1 score is : 1.0\n",
            "67\n",
            "Entity Level Mean F1 score is : 0.0\n",
            "68\n",
            "Entity Level Mean F1 score is : 0.0\n",
            "69\n",
            "Entity Level Mean F1 score is : 0.5\n",
            "70\n",
            "Entity Level Mean F1 score is : 0.0\n",
            "71\n",
            "Entity Level Mean F1 score is : 0.0\n",
            "72\n",
            "Entity Level Mean F1 score is : 0.75\n",
            "73\n",
            "Entity Level Mean F1 score is : 1.0\n",
            "74\n",
            "label starts with I! Impossible: from indices 83642\n",
            "Entity Level Mean F1 score is : 0.14285714285714288\n",
            "75\n",
            "Entity Level Mean F1 score is : 0.0\n",
            "76\n",
            "Entity Level Mean F1 score is : 0.0\n",
            "77\n",
            "Entity Level Mean F1 score is : 1.0\n",
            "78\n",
            "Entity Level Mean F1 score is : 1.0\n",
            "79\n",
            "Entity Level Mean F1 score is : 0.4444444444444444\n",
            "80\n",
            "Entity Level Mean F1 score is : 1.0\n",
            "81\n",
            "Entity Level Mean F1 score is : 0.0\n",
            "82\n",
            "Entity Level Mean F1 score is : 0.0\n",
            "83\n",
            "Entity Level Mean F1 score is : 0.25\n",
            "84\n",
            "Entity Level Mean F1 score is : 0.2222222222222222\n",
            "85\n",
            "Entity Level Mean F1 score is : 1.0\n",
            "86\n",
            "Entity Level Mean F1 score is : 1.0\n",
            "87\n",
            "Entity Level Mean F1 score is : 0.5\n",
            "88\n",
            "Entity Level Mean F1 score is : 1.0\n",
            "89\n",
            "Entity Level Mean F1 score is : 0.0\n",
            "90\n",
            "Entity Level Mean F1 score is : 0.0\n",
            "91\n",
            "Entity Level Mean F1 score is : 0.0\n",
            "92\n",
            "Entity Level Mean F1 score is : 0.0\n",
            "93\n",
            "Entity Level Mean F1 score is : 1.0\n",
            "94\n",
            "label starts with I! Impossible: from indices 79342\n",
            "label starts with I! Impossible: from indices 79345\n",
            "label starts with I! Impossible: from indices 79349\n",
            "Entity Level Mean F1 score is : 0.0\n",
            "95\n",
            "label starts with I! Impossible: from indices 246097\n",
            "Entity Level Mean F1 score is : 1.0\n",
            "96\n",
            "Entity Level Mean F1 score is : 0.0\n",
            "97\n",
            "Entity Level Mean F1 score is : 0.0\n",
            "98\n",
            "Entity Level Mean F1 score is : 0.0\n",
            "99\n",
            "Entity Level Mean F1 score is : 1.0\n",
            "100\n",
            "label starts with I! Impossible: from indices 129408\n",
            "label starts with I! Impossible: from indices 129408\n",
            "Entity Level Mean F1 score is : 1.0\n",
            "101\n",
            "Entity Level Mean F1 score is : 0.0\n",
            "102\n",
            "Entity Level Mean F1 score is : 0.0\n",
            "103\n",
            "Entity Level Mean F1 score is : 1.0\n",
            "104\n",
            "Entity Level Mean F1 score is : 1.0\n",
            "105\n",
            "Entity Level Mean F1 score is : 1.0\n",
            "106\n",
            "label starts with I! Impossible: from indices 165377\n",
            "Entity Level Mean F1 score is : 0.0\n",
            "107\n",
            "Entity Level Mean F1 score is : 0.0\n",
            "108\n",
            "Entity Level Mean F1 score is : 1.0\n",
            "109\n",
            "Entity Level Mean F1 score is : 0.5\n",
            "110\n",
            "Entity Level Mean F1 score is : 0.6666666666666666\n",
            "111\n",
            "Entity Level Mean F1 score is : 0.0\n",
            "112\n",
            "Entity Level Mean F1 score is : 0.0\n",
            "113\n",
            "Entity Level Mean F1 score is : 0.0\n",
            "114\n",
            "Entity Level Mean F1 score is : 0.8\n",
            "115\n",
            "Entity Level Mean F1 score is : 0.8333333333333333\n",
            "116\n",
            "Entity Level Mean F1 score is : 0.0\n",
            "117\n",
            "Entity Level Mean F1 score is : 0.0\n",
            "118\n",
            "Entity Level Mean F1 score is : 0.0\n",
            "119\n",
            "Entity Level Mean F1 score is : 0.0\n",
            "120\n",
            "Entity Level Mean F1 score is : 0.0\n",
            "121\n",
            "Entity Level Mean F1 score is : 1.0\n",
            "122\n",
            "Entity Level Mean F1 score is : 0.6666666666666665\n",
            "123\n",
            "Entity Level Mean F1 score is : 0.0\n",
            "124\n",
            "Entity Level Mean F1 score is : 1.0\n",
            "125\n",
            "Entity Level Mean F1 score is : 1.0\n",
            "126\n",
            "Entity Level Mean F1 score is : 1.0\n",
            "127\n",
            "label starts with I! Impossible: from indices 41910\n",
            "Entity Level Mean F1 score is : 0.5\n",
            "128\n",
            "Entity Level Mean F1 score is : 0.0\n",
            "129\n",
            "Entity Level Mean F1 score is : 0.0\n",
            "130\n",
            "label starts with I! Impossible: from indices 160962\n",
            "Entity Level Mean F1 score is : 0.75\n",
            "131\n",
            "Entity Level Mean F1 score is : 1.0\n",
            "132\n",
            "Entity Level Mean F1 score is : 1.0\n",
            "133\n",
            "Entity Level Mean F1 score is : 0.5\n",
            "134\n",
            "Entity Level Mean F1 score is : 1.0\n",
            "135\n",
            "Entity Level Mean F1 score is : 0.0\n",
            "136\n",
            "Entity Level Mean F1 score is : 0.0\n",
            "137\n",
            "Entity Level Mean F1 score is : 1.0\n",
            "138\n",
            "Entity Level Mean F1 score is : 0.0\n",
            "139\n",
            "Entity Level Mean F1 score is : 0.0\n",
            "140\n",
            "Entity Level Mean F1 score is : 0.5\n",
            "141\n",
            "Entity Level Mean F1 score is : 0.0\n",
            "142\n",
            "Entity Level Mean F1 score is : 0.6666666666666666\n",
            "143\n",
            "Entity Level Mean F1 score is : 1.0\n",
            "144\n",
            "Entity Level Mean F1 score is : 0.0\n",
            "145\n",
            "Entity Level Mean F1 score is : 0.0\n",
            "146\n",
            "Entity Level Mean F1 score is : 1.0\n",
            "147\n",
            "Entity Level Mean F1 score is : 0.0\n",
            "148\n",
            "Entity Level Mean F1 score is : 1.0\n",
            "149\n",
            "Entity Level Mean F1 score is : 0.5\n",
            "150\n",
            "Entity Level Mean F1 score is : 0.0\n",
            "151\n",
            "Entity Level Mean F1 score is : 1.0\n",
            "152\n",
            "Entity Level Mean F1 score is : 0.0\n",
            "153\n",
            "label starts with I! Impossible: from indices 227976\n",
            "Entity Level Mean F1 score is : 0.5\n",
            "154\n",
            "Entity Level Mean F1 score is : 0.0\n",
            "155\n",
            "Entity Level Mean F1 score is : 0.0\n",
            "156\n",
            "Entity Level Mean F1 score is : 0.0\n",
            "157\n",
            "Entity Level Mean F1 score is : 0.0\n",
            "158\n",
            "label starts with I! Impossible: from indices 181525\n",
            "Entity Level Mean F1 score is : 0.0\n",
            "159\n",
            "Entity Level Mean F1 score is : 0.0\n",
            "160\n",
            "Entity Level Mean F1 score is : 1.0\n",
            "161\n",
            "Entity Level Mean F1 score is : 1.0\n",
            "162\n",
            "label starts with I! Impossible: from indices 128040\n",
            "Entity Level Mean F1 score is : 0.0\n",
            "163\n",
            "Entity Level Mean F1 score is : 1.0\n",
            "164\n",
            "Entity Level Mean F1 score is : 0.923076923076923\n",
            "165\n",
            "Entity Level Mean F1 score is : 1.0\n",
            "166\n",
            "Entity Level Mean F1 score is : 1.0\n",
            "167\n",
            "Entity Level Mean F1 score is : 0.0\n",
            "168\n",
            "Entity Level Mean F1 score is : 0.0\n",
            "169\n",
            "Entity Level Mean F1 score is : 0.0\n",
            "170\n",
            "Entity Level Mean F1 score is : 0.0\n",
            "171\n",
            "Entity Level Mean F1 score is : 0.0\n",
            "172\n",
            "Entity Level Mean F1 score is : 0.25\n",
            "173\n",
            "Entity Level Mean F1 score is : 0.6666666666666666\n",
            "174\n",
            "Entity Level Mean F1 score is : 1.0\n",
            "175\n",
            "Entity Level Mean F1 score is : 0.5\n",
            "176\n",
            "Entity Level Mean F1 score is : 0.0\n",
            "177\n",
            "Entity Level Mean F1 score is : 0.0\n",
            "178\n",
            "Entity Level Mean F1 score is : 0.2222222222222222\n",
            "179\n",
            "Entity Level Mean F1 score is : 1.0\n",
            "180\n",
            "Entity Level Mean F1 score is : 1.0\n",
            "181\n",
            "Entity Level Mean F1 score is : 0.5\n",
            "182\n",
            "Entity Level Mean F1 score is : 1.0\n",
            "183\n",
            "Entity Level Mean F1 score is : 0.0\n",
            "184\n",
            "Entity Level Mean F1 score is : 1.0\n",
            "185\n",
            "Entity Level Mean F1 score is : 0.0\n",
            "186\n",
            "Entity Level Mean F1 score is : 0.5833333333333334\n",
            "187\n",
            "Entity Level Mean F1 score is : 0.0\n",
            "188\n",
            "Entity Level Mean F1 score is : 0.0\n",
            "189\n",
            "label starts with I! Impossible: from indices 266306\n",
            "Entity Level Mean F1 score is : 0.0\n",
            "190\n",
            "Entity Level Mean F1 score is : 0.0\n",
            "191\n",
            "Entity Level Mean F1 score is : 1.0\n",
            "192\n",
            "Entity Level Mean F1 score is : 0.0\n",
            "193\n",
            "Entity Level Mean F1 score is : 0.0\n",
            "194\n",
            "Entity Level Mean F1 score is : 0.5\n",
            "195\n",
            "Entity Level Mean F1 score is : 1.0\n",
            "196\n",
            "Entity Level Mean F1 score is : 0.0\n",
            "197\n",
            "Entity Level Mean F1 score is : 0.0\n",
            "198\n",
            "Entity Level Mean F1 score is : 0.0\n",
            "199\n",
            "Entity Level Mean F1 score is : 0.0\n",
            "200\n",
            "Entity Level Mean F1 score is : 0.0\n",
            "201\n",
            "Entity Level Mean F1 score is : 1.0\n",
            "202\n",
            "Entity Level Mean F1 score is : 0.0\n",
            "203\n",
            "Entity Level Mean F1 score is : 0.5\n",
            "204\n",
            "Entity Level Mean F1 score is : 0.0\n",
            "205\n",
            "Entity Level Mean F1 score is : 0.0\n",
            "206\n",
            "Entity Level Mean F1 score is : 0.3333333333333333\n",
            "207\n",
            "Entity Level Mean F1 score is : 0.0\n",
            "208\n",
            "Entity Level Mean F1 score is : 1.0\n",
            "209\n",
            "Entity Level Mean F1 score is : 1.0\n",
            "210\n",
            "Entity Level Mean F1 score is : 0.0\n",
            "211\n",
            "Entity Level Mean F1 score is : 0.0\n",
            "212\n",
            "Entity Level Mean F1 score is : 0.0\n",
            "213\n",
            "Entity Level Mean F1 score is : 0.0\n",
            "214\n",
            "label starts with I! Impossible: from indices 179960\n",
            "Entity Level Mean F1 score is : 0.0\n",
            "215\n",
            "Entity Level Mean F1 score is : 0.0\n",
            "216\n",
            "Entity Level Mean F1 score is : 1.0\n",
            "217\n",
            "Entity Level Mean F1 score is : 1.0\n",
            "218\n",
            "Entity Level Mean F1 score is : 1.0\n",
            "219\n",
            "Entity Level Mean F1 score is : 0.0\n",
            "220\n",
            "Entity Level Mean F1 score is : 0.0\n",
            "221\n",
            "label starts with I! Impossible: from indices 251599\n",
            "Entity Level Mean F1 score is : 0.0\n",
            "222\n",
            "Entity Level Mean F1 score is : 0.0\n",
            "223\n",
            "Entity Level Mean F1 score is : 0.0\n",
            "224\n",
            "Entity Level Mean F1 score is : 1.0\n",
            "225\n",
            "Entity Level Mean F1 score is : 0.0\n",
            "226\n",
            "Entity Level Mean F1 score is : 1.0\n",
            "227\n",
            "label starts with I! Impossible: from indices 229474\n",
            "Entity Level Mean F1 score is : 0.0\n",
            "228\n",
            "Entity Level Mean F1 score is : 1.0\n",
            "229\n",
            "Entity Level Mean F1 score is : 1.0\n",
            "230\n",
            "Entity Level Mean F1 score is : 0.0\n",
            "231\n",
            "Entity Level Mean F1 score is : 0.0\n",
            "232\n",
            "Entity Level Mean F1 score is : 1.0\n",
            "233\n",
            "Entity Level Mean F1 score is : 0.0\n",
            "234\n",
            "Entity Level Mean F1 score is : 0.0\n",
            "235\n",
            "Entity Level Mean F1 score is : 0.0\n",
            "236\n",
            "Entity Level Mean F1 score is : 0.0\n",
            "237\n",
            "Entity Level Mean F1 score is : 0.0\n",
            "238\n",
            "Entity Level Mean F1 score is : 0.0\n",
            "239\n",
            "Entity Level Mean F1 score is : 0.0\n",
            "240\n",
            "Entity Level Mean F1 score is : 1.0\n",
            "241\n",
            "Entity Level Mean F1 score is : 1.0\n",
            "242\n",
            "Entity Level Mean F1 score is : 0.0\n",
            "243\n",
            "Entity Level Mean F1 score is : 0.0\n",
            "244\n",
            "Entity Level Mean F1 score is : 1.0\n",
            "245\n",
            "Entity Level Mean F1 score is : 0.5\n",
            "246\n",
            "Entity Level Mean F1 score is : 0.0\n",
            "247\n",
            "Entity Level Mean F1 score is : 1.0\n",
            "248\n",
            "label starts with I! Impossible: from indices 182843\n",
            "Entity Level Mean F1 score is : 0.6666666666666666\n",
            "249\n",
            "Entity Level Mean F1 score is : 0.5714285714285715\n",
            "250\n",
            "Entity Level Mean F1 score is : 0.0\n",
            "251\n",
            "Entity Level Mean F1 score is : 0.0\n",
            "252\n",
            "Entity Level Mean F1 score is : 1.0\n",
            "253\n",
            "Entity Level Mean F1 score is : 1.0\n",
            "254\n",
            "Entity Level Mean F1 score is : 0.0\n",
            "255\n",
            "Entity Level Mean F1 score is : 1.0\n",
            "256\n",
            "Entity Level Mean F1 score is : 1.0\n",
            "257\n",
            "Entity Level Mean F1 score is : 1.0\n",
            "258\n",
            "Entity Level Mean F1 score is : 0.0\n",
            "259\n",
            "Entity Level Mean F1 score is : 0.0\n",
            "260\n",
            "Entity Level Mean F1 score is : 0.5\n",
            "261\n",
            "Entity Level Mean F1 score is : 0.0\n",
            "262\n",
            "Entity Level Mean F1 score is : 0.0\n",
            "263\n",
            "Entity Level Mean F1 score is : 1.0\n",
            "264\n",
            "Entity Level Mean F1 score is : 1.0\n",
            "265\n",
            "Entity Level Mean F1 score is : 1.0\n",
            "266\n",
            "Entity Level Mean F1 score is : 0.0\n",
            "267\n",
            "Entity Level Mean F1 score is : 0.0\n",
            "268\n",
            "Entity Level Mean F1 score is : 0.0\n",
            "269\n",
            "Entity Level Mean F1 score is : 1.0\n",
            "270\n",
            "Entity Level Mean F1 score is : 0.6666666666666666\n",
            "271\n",
            "Entity Level Mean F1 score is : 0.0\n",
            "272\n",
            "Entity Level Mean F1 score is : 0.0\n",
            "273\n",
            "Entity Level Mean F1 score is : 0.0\n",
            "274\n",
            "Entity Level Mean F1 score is : 0.0\n",
            "275\n",
            "Entity Level Mean F1 score is : 1.0\n",
            "276\n",
            "Entity Level Mean F1 score is : 0.5\n",
            "277\n",
            "Entity Level Mean F1 score is : 1.0\n",
            "278\n",
            "Entity Level Mean F1 score is : 1.0\n",
            "279\n",
            "Entity Level Mean F1 score is : 1.0\n",
            "280\n",
            "Entity Level Mean F1 score is : 1.0\n",
            "281\n",
            "Entity Level Mean F1 score is : 0.0\n",
            "282\n",
            "Entity Level Mean F1 score is : 1.0\n",
            "283\n",
            "Entity Level Mean F1 score is : 0.0\n",
            "284\n",
            "Entity Level Mean F1 score is : 1.0\n",
            "285\n",
            "Entity Level Mean F1 score is : 0.0\n",
            "286\n",
            "Entity Level Mean F1 score is : 0.0\n",
            "287\n",
            "Entity Level Mean F1 score is : 0.0\n",
            "288\n",
            "Entity Level Mean F1 score is : 0.5\n",
            "289\n",
            "Entity Level Mean F1 score is : 1.0\n",
            "290\n",
            "Entity Level Mean F1 score is : 0.0\n",
            "291\n",
            "Entity Level Mean F1 score is : 1.0\n",
            "292\n",
            "Entity Level Mean F1 score is : 1.0\n",
            "293\n",
            "Entity Level Mean F1 score is : 0.25\n",
            "294\n",
            "Entity Level Mean F1 score is : 1.0\n",
            "295\n",
            "Entity Level Mean F1 score is : 1.0\n",
            "296\n",
            "Entity Level Mean F1 score is : 0.0\n",
            "297\n",
            "Entity Level Mean F1 score is : 0.0\n",
            "298\n",
            "Entity Level Mean F1 score is : 0.0\n",
            "299\n",
            "Entity Level Mean F1 score is : 1.0\n",
            "300\n",
            "Entity Level Mean F1 score is : 0.0\n",
            "301\n",
            "Entity Level Mean F1 score is : 1.0\n",
            "302\n",
            "Entity Level Mean F1 score is : 0.8333333333333333\n",
            "303\n",
            "Entity Level Mean F1 score is : 0.8333333333333333\n",
            "304\n",
            "Entity Level Mean F1 score is : 0.0\n",
            "305\n",
            "Entity Level Mean F1 score is : 1.0\n",
            "306\n",
            "Entity Level Mean F1 score is : 0.2222222222222222\n",
            "307\n",
            "Entity Level Mean F1 score is : 0.0\n",
            "308\n",
            "label starts with I! Impossible: from indices 47939\n",
            "label starts with I! Impossible: from indices 47954\n",
            "Entity Level Mean F1 score is : 0.5555555555555555\n",
            "309\n",
            "Entity Level Mean F1 score is : 0.0\n",
            "310\n",
            "Entity Level Mean F1 score is : 0.6666666666666666\n",
            "311\n",
            "Entity Level Mean F1 score is : 1.0\n",
            "312\n",
            "Entity Level Mean F1 score is : 1.0\n",
            "313\n",
            "Entity Level Mean F1 score is : 1.0\n",
            "314\n",
            "Entity Level Mean F1 score is : 0.0\n",
            "315\n",
            "Entity Level Mean F1 score is : 0.0\n",
            "316\n",
            "label starts with I! Impossible: from indices 38712\n",
            "label starts with I! Impossible: from indices 38726\n",
            "Entity Level Mean F1 score is : 0.0\n",
            "317\n",
            "Entity Level Mean F1 score is : 0.25\n",
            "318\n",
            "Entity Level Mean F1 score is : 0.0\n",
            "319\n",
            "Entity Level Mean F1 score is : 0.0\n",
            "320\n",
            "Entity Level Mean F1 score is : 0.0\n",
            "321\n",
            "Entity Level Mean F1 score is : 0.0\n",
            "322\n",
            "Entity Level Mean F1 score is : 1.0\n",
            "323\n",
            "Entity Level Mean F1 score is : 0.0\n",
            "324\n",
            "label starts with I! Impossible: from indices 19175\n",
            "Entity Level Mean F1 score is : 0.0\n",
            "325\n",
            "Entity Level Mean F1 score is : 0.6666666666666666\n",
            "326\n",
            "Entity Level Mean F1 score is : 0.5\n",
            "327\n",
            "Entity Level Mean F1 score is : 1.0\n",
            "328\n",
            "Entity Level Mean F1 score is : 1.0\n",
            "329\n",
            "label starts with I! Impossible: from indices 76231\n",
            "Entity Level Mean F1 score is : 0.5\n",
            "330\n",
            "Entity Level Mean F1 score is : 0.0\n",
            "331\n",
            "Entity Level Mean F1 score is : 1.0\n",
            "332\n",
            "Entity Level Mean F1 score is : 1.0\n",
            "333\n",
            "Entity Level Mean F1 score is : 1.0\n",
            "334\n",
            "label starts with I! Impossible: from indices 10809\n",
            "label starts with I! Impossible: from indices 10811\n",
            "label starts with I! Impossible: from indices 10814\n",
            "Entity Level Mean F1 score is : 0.0\n",
            "335\n",
            "Entity Level Mean F1 score is : 0.5\n",
            "336\n",
            "Entity Level Mean F1 score is : 0.6666666666666666\n",
            "337\n",
            "Entity Level Mean F1 score is : 0.0\n",
            "338\n",
            "Entity Level Mean F1 score is : 0.5\n",
            "339\n",
            "Entity Level Mean F1 score is : 1.0\n",
            "340\n",
            "Entity Level Mean F1 score is : 1.0\n",
            "341\n",
            "Entity Level Mean F1 score is : 0.0\n",
            "342\n",
            "Entity Level Mean F1 score is : 0.0\n",
            "343\n",
            "Entity Level Mean F1 score is : 0.0\n",
            "344\n",
            "Entity Level Mean F1 score is : 1.0\n",
            "345\n",
            "Entity Level Mean F1 score is : 1.0\n",
            "346\n",
            "Entity Level Mean F1 score is : 1.0\n",
            "347\n",
            "Entity Level Mean F1 score is : 0.4\n",
            "348\n",
            "Entity Level Mean F1 score is : 0.5\n",
            "349\n",
            "Entity Level Mean F1 score is : 0.5\n",
            "350\n",
            "Entity Level Mean F1 score is : 1.0\n",
            "351\n",
            "Entity Level Mean F1 score is : 1.0\n",
            "352\n",
            "Entity Level Mean F1 score is : 1.0\n",
            "353\n",
            "Entity Level Mean F1 score is : 0.0\n",
            "354\n",
            "Entity Level Mean F1 score is : 0.5\n",
            "355\n",
            "Entity Level Mean F1 score is : 0.0\n",
            "356\n",
            "Entity Level Mean F1 score is : 0.0\n",
            "357\n",
            "Entity Level Mean F1 score is : 0.0\n",
            "358\n",
            "Entity Level Mean F1 score is : 1.0\n",
            "359\n",
            "Entity Level Mean F1 score is : 1.0\n",
            "360\n",
            "Entity Level Mean F1 score is : 0.0\n",
            "361\n",
            "Entity Level Mean F1 score is : 0.6666666666666666\n",
            "362\n",
            "Entity Level Mean F1 score is : 0.0\n",
            "363\n",
            "Entity Level Mean F1 score is : 1.0\n",
            "364\n",
            "Entity Level Mean F1 score is : 0.0\n",
            "365\n",
            "Entity Level Mean F1 score is : 0.6666666666666666\n",
            "366\n",
            "Entity Level Mean F1 score is : 0.0\n",
            "367\n",
            "Entity Level Mean F1 score is : 0.4444444444444445\n",
            "368\n",
            "Entity Level Mean F1 score is : 0.0\n",
            "369\n",
            "Entity Level Mean F1 score is : 1.0\n",
            "370\n",
            "Entity Level Mean F1 score is : 1.0\n",
            "371\n",
            "Entity Level Mean F1 score is : 1.0\n",
            "372\n",
            "Entity Level Mean F1 score is : 0.0\n",
            "373\n",
            "Entity Level Mean F1 score is : 0.0\n",
            "374\n",
            "Entity Level Mean F1 score is : 0.0\n",
            "375\n",
            "Entity Level Mean F1 score is : 1.0\n",
            "376\n",
            "Entity Level Mean F1 score is : 1.0\n",
            "377\n",
            "Entity Level Mean F1 score is : 0.5\n",
            "378\n",
            "Entity Level Mean F1 score is : 0.0\n",
            "379\n",
            "Entity Level Mean F1 score is : 0.0\n",
            "380\n",
            "Entity Level Mean F1 score is : 0.0\n",
            "381\n",
            "Entity Level Mean F1 score is : 1.0\n",
            "382\n",
            "Entity Level Mean F1 score is : 0.0\n",
            "383\n",
            "Entity Level Mean F1 score is : 0.0\n",
            "384\n",
            "Entity Level Mean F1 score is : 0.0\n",
            "385\n",
            "Entity Level Mean F1 score is : 1.0\n",
            "386\n",
            "Entity Level Mean F1 score is : 1.0\n",
            "387\n",
            "Entity Level Mean F1 score is : 1.0\n",
            "388\n",
            "Entity Level Mean F1 score is : 0.0\n",
            "389\n",
            "Entity Level Mean F1 score is : 0.0\n",
            "390\n",
            "Entity Level Mean F1 score is : 0.0\n",
            "391\n",
            "Entity Level Mean F1 score is : 0.0\n",
            "392\n",
            "Entity Level Mean F1 score is : 0.0\n",
            "393\n",
            "Entity Level Mean F1 score is : 1.0\n",
            "394\n",
            "Entity Level Mean F1 score is : 0.0\n",
            "395\n",
            "Entity Level Mean F1 score is : 0.0\n",
            "396\n",
            "Entity Level Mean F1 score is : 1.0\n",
            "397\n",
            "Entity Level Mean F1 score is : 0.0\n",
            "398\n",
            "Entity Level Mean F1 score is : 1.0\n",
            "399\n",
            "label starts with I! Impossible: from indices 110952\n",
            "Entity Level Mean F1 score is : 0.3333333333333333\n"
          ]
        }
      ],
      "source": [
        "mean_f1_list = []\n",
        "for i in range(len(pred_token_labels_ll)):\n",
        "  print(i)\n",
        "  y_pred_dict = format_output_labels(pred_token_labels_ll[i], token_indices[i])\n",
        "  #print(\"y_pred_dict is : \" + str(y_pred_dict))\n",
        "  y_true_dict = format_output_labels(true_token_labels_ll[i], token_indices[i])\n",
        "  #print(\"y_true_dict is : \" + str(y_true_dict))\n",
        "  cur_mean_f1 = mean_f1(y_pred_dict, y_true_dict)\n",
        "  mean_f1_list.append(cur_mean_f1)\n",
        "  print(\"Entity Level Mean F1 score is : \" + str(cur_mean_f1))\n",
        "mean_mean_f1 = sum(mean_f1_list)/len(mean_f1_list)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def variance(data):\n",
        "  n = len(data)\n",
        "  mean = sum(data) / n\n",
        "  deviations = [(x - mean) ** 2 for x in data]\n",
        "  variance = sum(deviations) / n\n",
        "  return variance"
      ],
      "metadata": {
        "id": "LhSDZJVgKAcI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YRLi5TDyuSx_"
      },
      "source": [
        "<a name=\"q2.1\"></a>\n",
        "[[^^^]](#outline) \n",
        "## **Q2.1: Explain your HMM Implementations**\n",
        "\n",
        "Explain how you implemented the HMM including the Viterbi algorithm. Make clear which parts were implemented from scratch vs. obtained via an existing package (review the Logistics section for information on packages that are not allowed). Explain and motivate any design choices providing the intuition behind them.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sL2bUPVkNpbt"
      },
      "source": [
        "#### **A2.1:**\n",
        "\n",
        "For the HMM implementations, we implement all functions including build_transitive_matrix, build_emission_matrix, get_start_prob, build_hmm, and viterbi from scratch. \n",
        "\n",
        "The reason we conduct these functions from scratch is that the math logics and processes used to implement HMM model and viterbi are super clear.\n",
        "\n",
        "Here are our step for implememtations:\n",
        "1. build transitive matrix: we first build a new list; append an end string in every NER list in train_data; append this NER list into this new list. This is to make sure we also consider the situation words go from NER to the end of the whole sentence (this NER is the last NER in a NER list). Next, we create a new dictionary, for each NER list, we find each tuple of previous NER and cur NER/end and treat them as the key of this new dictionary and add 1 each time we go through a tuple to the value of the related key and the total amount. Finally, we will divide every value (with k smoothing) by total amount (with k*len(label_list) smoothing) and make them log to get the transitive matrix.\n",
        "2. build emission matrix: we first build sets of tokens and label gotten from whole train_data. Next, create a new dictionary, use tuple of label and its related token as keys; every time we meet one pair we add 1 to its related value (using a nested loop) and to a total amount. Finally, we will divide every value (with k smoothing) by total amount (with k*len(label_list) smoothing) and make them log to get the transitive matrix.\n",
        "3. build start_prob: We first creat a map and put all label list into it with a default value k(smoothing), then we loop to count the first label. Then calculate the starting prob for each label\n",
        "4. build_hmm: we wrapped up all three matrix togather with tokens_list and labels_list and return\n",
        "5. vertibi:We first handled the OOV words in observation. We then define a dp List[List[int]], initialize it with the starting probs * emission probs and same data structure as backpointer. We then first loop the observation, second loop the pre label list and third loop the current label list, for each loop we multiply the emission and transition together with dp matrix and we track the backpointer as well. In the end, we start from the largest prob in dp, backtrack the pointer save in a list. Finally, we return the reverse list"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DHbzRuil-yjG"
      },
      "source": [
        "<a name=\"q2.2\"></a>\n",
        "[[^^^]](#outline) \n",
        "## **Q2.2: Results Analysis**\n",
        "\n",
        "Explain here how you evaluated the models. Summarize the performance of your system and any variations that you experimented with on the validation datasets. Put the results into clearly labeled tables or diagrams and include your observations and analysis.\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(variance(mean_f1_list))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T0rkaCUK5_lH",
        "outputId": "2f29dfb9-adb4-4035-d541-ee36095925f8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.20597810045269466\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j_aBZqFKOKRO"
      },
      "source": [
        "#### **A2.2:**\n",
        "\n",
        "We use F1 score to evaluated the model. when F1 = 1, the model can perfectly predict the BOI tag of each tokens in validation data (predicted BOI same as provided BOI). our mean f1 is about 0.459 and variance is about 0.206."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rTIPGnLFNc43"
      },
      "source": [
        "<a name=\"q2.3\"></a>\n",
        "[[^^^]](#outline) \n",
        "## **Q2.3: Error Analysis**\n",
        "When did the system work well? When did it fail?  Any ideas as to why? How might you improve the system?\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u7j0YVozOMIT"
      },
      "source": [
        "#### **A2.3:**\n",
        "\n",
        "In the validation part, we can find model fails many times when there are \"Unknown\" words which are set \"UNK\". However, when there is little or not \"UNK\" in an observation, the system works very well. the main ideas that why this will happens is because of the unknown word handling. In our version of unknown word handling, we simply decides all words occuring only one time in training set and its length larger than 12 as unknown words. However, there is not difference between these \"unknown\" word although we have assign them different \"BOI\" tag in our training data. In addition, with the logic to develop hmm model, transitive probability has a great impact on our prediction. With most of \"unknown\" words set as \"O\" (because most of words should be \"O\" as we all know), the hmm model will easily set the label of a unknown word as \"O\".\n",
        "\n",
        "To improve this system, I will divide all \"unknown\" words into different types, such as: \"unknown-O\", \"unknown-B-per\", \"unknown-I-per\" etc. based on the related \"BIO\" tags provided in our training \"NER\" list."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mf6ziT36NteS"
      },
      "source": [
        "<a name=\"q2.4\"></a>\n",
        "[[^^^]](#outline) \n",
        "## **Q2.4: What is the effect of unknown word handling and smoothing?**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q9PL6823OQPT"
      },
      "source": [
        "#### **A2.4:**\n",
        "For unknown words handling: \n",
        " - We select the unknown words by words frequency, we set threshold to be 1, but turn out there still near half be UNK, so we further limited by length of words, len(w)>12 will be seleted\n",
        " - The effect of unknown word handling is to avoid out of vocabulary words. When we test or validate the model, if some words is not in the train vocabulary, there would be no corresponding prob in emission matrix, and would cause error.\n",
        "\n",
        "For smoothing:\n",
        "  - We choose to use add-k smoothing algrithms. For k value, we tried from 0.01- 1 with the best combination. \n",
        "  - The effect of smoothing is to avoid the unseen event to assigned a 0 prob. Like there is no \"B-PER->O\" in training set, the prob is set 0, and when we do the vertibi, this 0 prob will cause the product result to be 0.\n",
        " "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "31e3sMHZrLWP"
      },
      "source": [
        "<a name=\"part3\"></a>\n",
        "# **Part 3: Maximum Entropy Markov Model** \n",
        "[[^^^]](#outline) \n",
        "---\n",
        "\n",
        "In this section, you will implement a Maximum Entropy Markov Model (**MEMM**) to perform the same NER task. Your model should consist of a MaxEnt classifier with Viterbi decoding. \n",
        "\n",
        "1. We have already performed tokenizations for documents. You can either use a MaxEnt classifier from an existing package or write the MaxEnt code yourself. **Important note:  MaxEnt classifiers are statistically equivalent to multi-class logistic regression, so you can use packages for multi-class LR instead of MaxEnt.**\n",
        "\n",
        "2. Use the classifier to learn a probability $P(t_i|features)$. You may replace either the lexical generation probability  $P(w_i|t_i)$  or the transition probability  $P(t_i|t_{i1})$  in the HMM with it, or you may replace the entire *lexical generation probability * transition probability*  calculation  $P (w_i|t_i)  P (t_i|t_{i1)}  $ in the HMM with it. \n",
        "\n",
        "3. To train such a classifier, you need to pick some feature set. The content of the feature set is up to you, but try different ones, and evaluate your choices on the validation set. Pick the feature set that performs overall the best according to the F1 measure. If you draw inspiration for your features from external sources, please link them in the code.\n",
        "  * While there are many directions to take when looking for features, you may start by exploring parts of speech that appear in sentences. There are several libraries (ex. [nltk](https://www.nltk.org/book/ch05.html)) that process sentences and identify parts of speech. If you end up using a library to extract parts of speech tags or other features, please indicate this in your asnwer to Q3.1.\n",
        "\n",
        "4. Use your own implementation of the **Viterbi algorithm**, which you can modify from the one you developed for the HMM model. You will need the probabilities that you obtain from the MaxEnt classifier. \n",
        "\n",
        "5. Remember to use same training and validation split when evaluating the MEMM to have a **fair comparison** with your **HMM model**.\n",
        "\n",
        "\n",
        "Please also take a look into your misclassified cases, as we will be performing error analysis in Part 4. \n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PJIosHVJZ-1o"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "Here's a summary of the workflow for Part 3:\n",
        "\n",
        "![alt text](https://drive.google.com/uc?export=view&id=14VfjW3yDyXLojWM_u0LeJYdDOSLkElBn)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8QGSijPUW_Bi"
      },
      "source": [
        "Note that we have not provided any skeleton code for how you should do feature engineering since this is meant to be an open ended task and we want you to experiment with the dataset. However, please remember to make sure that you code is concise, clean, and readable! Ultimately, we expect a function or class  mapping a sequence of tokens to a sequence of labels. At the end of this section you should have done the following:\n",
        "1. Extract Features\n",
        "2. Build & Train MaxEnt\n",
        "3. Call Viterbi when evaluating\n",
        "\n",
        "### References\n",
        "You may find [chapter 8](https://web.stanford.edu/~jurafsky/slp3/8.pdf) of Jurafsky and Martin book useful. In particular, you could consider section 8.5.2 for features in NER. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pd2PwG4wYkhQ"
      },
      "source": [
        "<a name=\"features\"></a>\n",
        "## **Feature Engineering**\n",
        "[[^^^]](#outline) \n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6xBYPGLUHH7n",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2afc2a9e-ce44-4ee0-ed35-289807ad80cf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n"
          ]
        }
      ],
      "source": [
        "# Your implementation here\n",
        "# w.istitle(), w.islower(), w.isupper(), len(w), w.isdigit(), w.isalpha(), \n",
        "# is_first_upper(), is_pre1_first_upper(), is_pre2_first_upper(),  is_next1_first_upper()\n",
        "# is_next2_first_upper(), is_noun(current POS), is_pre_noun(previous POS), is_next_noun(next POS)\n",
        "# is_next_word_university, is_contains_number, gazetter\n",
        "import nltk\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "def pos_tag(tokens):\n",
        "  pos=[]\n",
        "  for token in tokens:\n",
        "    tmp=[]\n",
        "    token_tagged =nltk.pos_tag(token)\n",
        "    \n",
        "    for t in token_tagged:\n",
        "      tmp.append(t[1])\n",
        "    pos.append(tmp)\n",
        "  return pos\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_data['text']"
      ],
      "metadata": {
        "id": "k1MJAxrXdUwg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "30679d56-0545-475c-c34b-388a37a30b4a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0       [Because, of, its, high, rate, of, economic, m...\n",
              "1       [Every, aspect, of, life, was, regulated, to, ...\n",
              "2       [Yet, despite, his, impending, death, ,, Louis...\n",
              "3       [Direct, fluorescent, antibody, can, also, be,...\n",
              "4       [It, has, yet, to, be, released, on, CD, ,, th...\n",
              "                              ...                        \n",
              "6995    [The, Carboniferous, lycophytes, of, the, orde...\n",
              "6996    [On, 23, April, 2008, ,, Miles, Leonard, was, ...\n",
              "6997    [They, designed, everything, from, the, interi...\n",
              "6998    [In, honor, of, this, event, ,, the, Colonna, ...\n",
              "6999    [The, resulting, offspring, is, registered, in...\n",
              "Name: text, Length: 7000, dtype: object"
            ]
          },
          "metadata": {},
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VawYGZ4gqQ01",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fbfd1b76-b165-49ca-d266-11a364b137ef"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting geotext\n",
            "  Downloading geotext-0.4.0-py2.py3-none-any.whl (2.0 MB)\n",
            "\u001b[K     || 2.0 MB 28.0 MB/s \n",
            "\u001b[?25hInstalling collected packages: geotext\n",
            "Successfully installed geotext-0.4.0\n"
          ]
        }
      ],
      "source": [
        "!pip install geotext"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WtU7rKKeWsDJ"
      },
      "outputs": [],
      "source": [
        "from geotext import GeoText\n",
        "\n",
        "def extract_feature(tokens, pos):\n",
        "  res=[]\n",
        "  for i in range(len(tokens)):\n",
        "    tmp=[]\n",
        "    ll = len(tokens[i])\n",
        "    for j in range(ll):\n",
        "      cur_token= tokens[i][j]\n",
        "      cur_pos= pos[i][j]\n",
        "      feature= {\n",
        "      #     'gazetter_city': 1 if len(GeoText(cur_token).country_mentions) >0 else 0,\n",
        "      #     'gazetter_country': 1 if len(GeoText(cur_token).cities) >0 else 0,\n",
        "          #'is_nn': 1 if cur_pos == 'NN' else 0,\n",
        "          # #'is_nns': 1 if cur_pos == 'NNS' else 0,\n",
        "           #'is_nnp': 1 if cur_pos == 'NNP' else 0,\n",
        "          # 'is_nnps': 1 if cur_pos == 'NNPS' else 0,\n",
        "          # 'is_nns': 1 if cur_pos == 'NNS' else 0,\n",
        "          # 'is_pre_jj-': 1 if pos[i][j-1][:2] == 'JJ' and j>0 else 0,\n",
        "          # 'is_pre_DT': 1 if pos[i][j-1] == 'DT' and j>0 else 0,\n",
        "          # 'is_pre_vb': 1 if pos[i][j-1][:2] == 'VB' and j>0 else 0,\n",
        "          # 'is_pre_w': 1 if pos[i][j-1][:1] == 'W' and j>0 else 0,\n",
        "          #'is_fw': 1 if cur_pos == 'FW' else 0,\n",
        "          # 'is_jj': 1 if cur_pos == 'JJ' else 0,\n",
        "          #'is_first': 1 if j==0 else 0,\n",
        "          #'is_second': 1 if j==1 else 0,\n",
        "          #'is_last': 1 if j==len(tokens[i])-1 else 0,\n",
        "          #'is_last2': 1 if j==len(tokens[i])-2 else 0,\n",
        "          'is_title': 1 if cur_token.istitle() else 0,\n",
        "          #'is_lower': 1 if cur_token.islower() else 0,\n",
        "          #'is_digit': 1 if cur_token.isdigit() else 0,\n",
        "          #'is_alpha': 1 if cur_token.isalpha() else 0,\n",
        "          #'len': len(cur_token),\n",
        "           #'is_pre_the': 1 if j>0 and tokens[i][j-1]=='The' else 0,\n",
        "          # 'is_pre_the2': 1 if j>0 and tokens[i][j-1]=='the' else 0,\n",
        "           #'is_pre_at': 1 if j>0 and tokens[i][j-1]=='at' else 0,\n",
        "           #'is_pre_by': 1 if j>0 and tokens[i][j-1]=='by' else 0,\n",
        "          # 'is_pre_in': 1 if j>1 and tokens[i][j-1]=='in' else 0,\n",
        "          # 'is_pre_title': 1 if j>0 and tokens[i][j-1].istitle() else 0,\n",
        "          #'is_pre_jj': 1 if j>0 and pos[i][j-1]=='JJ' else 0,\n",
        "          # 'is_pre_nnp': 1 if j>0 and pos[i][j-1]=='NNP' else 0,\n",
        "          #\"start_token\": tokens[i][0],\n",
        "          #\"start_pos\": pos[i][0],\n",
        "          \"cur_pos\": cur_pos,\n",
        "          \"1_pre_pos\": pos[i][j-1] if j>0 else \"start pos\",\n",
        "          #\"next_pos\": tokens[i][j+1] if j<ll-1 else \"end pos\",\n",
        "          \"cur_token\": cur_token,\n",
        "          \"1_pre_token\": tokens[i][j-1] if j>0 else \"start token\",\n",
        "          #\"next_token\": tokens[i][j+1] if j<ll-1 else \"end token\"\n",
        "          #'is_next_nnp': 1 if j<len(pos[i])-1 and pos[i][j+1] == 'nnps' else 0,\n",
        "          #'is_next_title': 1 if j<len(tokens[i])-1 and tokens[i][j+1] == 'title' else 0,\n",
        "          #'is_pre_nnps': 1 if j>0 and pos[i][j-1]=='NNPS' else 0, \n",
        "          #'is_next_pos': 1 if j<len(pos[i])-1 and pos[i][j+1] == 'POS' else 0,\n",
        "          #'is_next_vb': 1 if j<len(pos[i])-1 and pos[i][j+1] == 'VB' else 0,\n",
        "          #'is_next_vbd': 1 if j<len(pos[i])-1 and pos[i][j+1] == 'VBD' else 0,\n",
        "          #'is_next_vbp': 1 if j<len(pos[i])-1 and pos[i][j+1] == 'VBP' else 0,\n",
        "          #'is_next_vbz': 1 if j<len(pos[i])-1 and pos[i][j+1] == 'VBZ' else 0,\n",
        "          #'is_next_university': 1 if j< len(tokens[i])-1 and tokens[i][j+1] =='University' else 0, # ?\n",
        "          #'is_pre_cornell': 1 if j>0 and tokens[i][j-1] == 'Cornell' else 0\n",
        "        \n",
        "      }\n",
        "      tmp.append(feature)\n",
        "    res.append(tmp)\n",
        "  return res     "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "agTrCtsCqagS"
      },
      "outputs": [],
      "source": [
        "def join_feature(features, labels):\n",
        "  joint_features= []\n",
        "  for i in range(len(features)):\n",
        "    tmp=[]\n",
        "    for j in range(len(features[i])):\n",
        "      tmp.append((features[i][j], labels[i][j]))\n",
        "    joint_features.append(tmp)\n",
        "  return joint_features"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jCkP4A3IYwit"
      },
      "source": [
        "<a name=\"memm_implementation\"></a>\n",
        "## **MEMM Implementation**\n",
        "[[^^^]](#outline) \n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JyMObKU3VgAl"
      },
      "outputs": [],
      "source": [
        "from nltk.classify import MaxentClassifier\n",
        "def memm(tokens, labels):\n",
        "  res={}\n",
        "  pos= pos_tag(tokens)\n",
        "  features= extract_feature(tokens, pos)\n",
        "  joint_features= join_feature(features, labels)\n",
        "  flat_featuers = [item for sublist in joint_features for item in sublist]\n",
        "  classifier = nltk.classify.MaxentClassifier.train(flat_featuers, trace=0, max_iter=10)\n",
        "  res[\"classifier\"]= classifier\n",
        "  label_map= {}\n",
        "  for la in labels:\n",
        "    for l in la:\n",
        "      label_map[l]= label_map.get(l, 0)+1\n",
        "  res[\"labels\"] = list(label_map.keys())\n",
        "  token_map= {}\n",
        "  for to in tokens:\n",
        "    for t in to:\n",
        "      token_map[t]= token_map.get(t, 0) +1\n",
        "  res[\"tokens\"]= list(token_map.keys())\n",
        "\n",
        "  return res"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iAR5IjDxY4GJ"
      },
      "outputs": [],
      "source": [
        "import sys\n",
        "\n",
        "\n",
        "def vertibi_memm(memm, observation):\n",
        "  memm_classifier= memm[\"classifier\"]\n",
        "  memm_tokens= memm[\"tokens\"]\n",
        "  memm_labels= memm[\"labels\"]\n",
        "  obs_tagged= nltk.pos_tag(observation)\n",
        "  pos_obs= []\n",
        "  for t in obs_tagged:\n",
        "    pos_obs.append(t[1])\n",
        "  pos_obs= [pos_obs]\n",
        "  observation= [observation]\n",
        "  # print(pos_obs)\n",
        "  # print(observation)\n",
        "  features_obs= extract_feature(observation, pos_obs)\n",
        "  flat_obs = [item for sublist in features_obs for item in sublist] # flat the features_obs\n",
        "  prob_obs=[]\n",
        "\n",
        "  for flat_obs in flat_obs:\n",
        "    prob= memm_classifier.prob_classify(flat_obs)\n",
        "    prob_obs.append(prob)\n",
        "  observation = [item for sublist in observation for item in sublist]\n",
        "  backpointer= [[0]*len(observation) for i in range(len(memm_labels))] # [label][obser]\n",
        "  dp= [[0]*len(observation) for i in range(len(memm_labels))]\n",
        "  for i in range(len(memm_labels)):\n",
        "    dp[i][0]= math.log10(prob_obs[0].prob(memm_labels[i]))\n",
        "\n",
        "  for i in range(1, len(observation)):\n",
        "    pointer=-sys.maxsize - 1\n",
        "    for j in range(len(memm_labels)):\n",
        "      m= -sys.maxsize - 1\n",
        "      for k in range(len(memm_labels)):\n",
        "        cur= dp[k][i-1] + math.log10(prob_obs[i].prob(memm_labels[j])) \n",
        "        if cur > m:\n",
        "          m= cur\n",
        "          backpointer[j][i]= k\n",
        "      dp[j][i]= m  \n",
        "  #print(dp)\n",
        "  m= -sys.maxsize - 1\n",
        "  idx= -1\n",
        "  for i in range(len(memm_labels)):\n",
        "    cur= dp[i][len(observation)-1]\n",
        "    if cur > m:\n",
        "      m= cur\n",
        "      idx= i  \n",
        "  res=[]\n",
        "  res.append(memm_labels[idx])\n",
        "  for j in range(len(observation)-1, 0, -1):\n",
        "    #print(backpointer[idx][j])\n",
        "    idx= backpointer[idx][j]\n",
        "    res.append(memm_labels[idx])\n",
        "\n",
        "  res.reverse()  \n",
        "  #print(res)\n",
        "  return res"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vvoeAVMlX4gp"
      },
      "source": [
        "### **Validation**\n",
        "---\n",
        "In this section we want you to run your MaxEnt model on the validation dataset you extracted earlier. We want you to play around with different combinations of features in order to find which features work the best for your implementation. You will be asked to write about this process in detail in written question 3.3 so please spend time experimenting with features! Once again, please use the code we provided for computing Entity Level Avg F1 earlier when validating your model."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# feature3= {\n",
        "#           'gazetter_city': 1 if len(GeoText(cur_token).country_mentions) >0 else 0,\n",
        "#           'gazetter_country': 1 if len(GeoText(cur_token).cities) >0 else 0,\n",
        "#           'is_nnp': 1 if cur_pos == 'NNP' else 0,\n",
        "#           'is_nnps': 1 if cur_pos == 'NNPS' else 0,\n",
        "#           'is_nns': 1 if cur_pos == 'NNS' else 0,\n",
        "#           'is_pre_jj-': 1 if pos[i][j-1][:2] == 'JJ' and j>0 else 0,\n",
        "#           'is_pre_DT': 1 if pos[i][j-1] == 'DT' and j>0 else 0,\n",
        "#           'is_pre_vb': 1 if pos[i][j-1][:2] == 'VB' and j>0 else 0,\n",
        "#           'is_pre_w': 1 if pos[i][j-1][:1] == 'W' and j>0 else 0,\n",
        "#           'is_title': 1 if cur_token.istitle() else 0,\n",
        "#           \"cur_token\": cur_token,\n",
        "#           \"1_pre_token\": tokens[i][j-1] if j>0 else \"start of a sentence\",\n",
        "#           \"next_token\": tokens[i][j+1] if j<ll-1 else \"end of a sentence\"\n",
        "#       }"
      ],
      "metadata": {
        "id": "v1U74YOSj1L3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TmUnuuxvHKXM"
      },
      "outputs": [],
      "source": [
        "# Run your model on validation set\n",
        "# You will need to \n",
        "# 1. Call your function above to get a prediction result on Validation Set\n",
        "# 2. Report Metrics\n",
        "# (See if you need to modify your feature set)\n",
        "\n",
        "memm_model= memm(train_data['text'], train_data['NER'])\n",
        "pred_token_labels_ll = []\n",
        "true_token_labels_ll = []\n",
        "token_indices = []\n",
        "for i in range(len(val_data['text'])):\n",
        "  pred_token_labels = vertibi_memm(memm_model, val_data['text'][i])\n",
        "  pred_token_labels_ll.append(pred_token_labels)\n",
        "  true_token_labels_ll.append(val_data['NER'][i])\n",
        "  token_indices.append(val_data['index'][i])"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# for i in range(len(pred_token_labels_ll)):\n",
        "#   print(i)\n",
        "#   y_pred_dict = format_output_labels(pred_token_labels_ll[i], token_indices[i])\n",
        "#   #print(\"y_pred_dict is : \" + str(y_pred_dict))\n",
        "#   y_true_dict = format_output_labels(true_token_labels_ll[i], token_indices[i])\n",
        "#   #print(\"y_true_dict is : \" + str(y_true_dict))\n",
        "#   print(\"Entity Level Mean F1 score is : \" + str(mean_f1(y_pred_dict, y_true_dict)))\n",
        "mean_f1_list = []\n",
        "for i in range(len(pred_token_labels_ll)):\n",
        "  print(i)\n",
        "  y_pred_dict = format_output_labels(pred_token_labels_ll[i], token_indices[i])\n",
        "  #print(\"y_pred_dict is : \" + str(y_pred_dict))\n",
        "  y_true_dict = format_output_labels(true_token_labels_ll[i], token_indices[i])\n",
        "  #print(\"y_true_dict is : \" + str(y_true_dict))\n",
        "  cur_mean_f1 = mean_f1(y_pred_dict, y_true_dict)\n",
        "  mean_f1_list.append(cur_mean_f1)\n",
        "  print(\"Entity Level Mean F1 score is : \" + str(cur_mean_f1))\n",
        "mean_mean_f1 = sum(mean_f1_list)/len(mean_f1_list)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rytP7x3s5Wyi",
        "outputId": "f8f3b5e9-0724-429d-a8e4-908fc9322159"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0\n",
            "Entity Level Mean F1 score is : 1.0\n",
            "1\n",
            "Entity Level Mean F1 score is : 0.0\n",
            "2\n",
            "Entity Level Mean F1 score is : 0.0\n",
            "3\n",
            "Entity Level Mean F1 score is : 0.5\n",
            "4\n",
            "Entity Level Mean F1 score is : 0.0\n",
            "5\n",
            "Entity Level Mean F1 score is : 1.0\n",
            "6\n",
            "Entity Level Mean F1 score is : 0.0\n",
            "7\n",
            "Entity Level Mean F1 score is : 0.0\n",
            "8\n",
            "Entity Level Mean F1 score is : 1.0\n",
            "9\n",
            "label starts with I! Impossible: from indices 117557\n",
            "Entity Level Mean F1 score is : 0.6666666666666666\n",
            "10\n",
            "Entity Level Mean F1 score is : 1.0\n",
            "11\n",
            "label starts with I! Impossible: from indices 139873\n",
            "Entity Level Mean F1 score is : 0.5\n",
            "12\n",
            "Entity Level Mean F1 score is : 1.0\n",
            "13\n",
            "Entity Level Mean F1 score is : 0.0\n",
            "14\n",
            "Entity Level Mean F1 score is : 0.0\n",
            "15\n",
            "Entity Level Mean F1 score is : 1.0\n",
            "16\n",
            "Entity Level Mean F1 score is : 0.0\n",
            "17\n",
            "Entity Level Mean F1 score is : 0.5\n",
            "18\n",
            "Entity Level Mean F1 score is : 1.0\n",
            "19\n",
            "Entity Level Mean F1 score is : 0.3333333333333333\n",
            "20\n",
            "Entity Level Mean F1 score is : 0.0\n",
            "21\n",
            "label starts with I! Impossible: from indices 175921\n",
            "Entity Level Mean F1 score is : 0.5\n",
            "22\n",
            "Entity Level Mean F1 score is : 0.0\n",
            "23\n",
            "label starts with I! Impossible: from indices 136713\n",
            "Entity Level Mean F1 score is : 0.0\n",
            "24\n",
            "label starts with I! Impossible: from indices 172688\n",
            "Entity Level Mean F1 score is : 0.0\n",
            "25\n",
            "Entity Level Mean F1 score is : 0.0\n",
            "26\n",
            "Entity Level Mean F1 score is : 0.0\n",
            "27\n",
            "Entity Level Mean F1 score is : 1.0\n",
            "28\n",
            "Entity Level Mean F1 score is : 0.0\n",
            "29\n",
            "Entity Level Mean F1 score is : 0.0\n",
            "30\n",
            "Entity Level Mean F1 score is : 0.0\n",
            "31\n",
            "Entity Level Mean F1 score is : 0.25\n",
            "32\n",
            "Entity Level Mean F1 score is : 0.0\n",
            "33\n",
            "Entity Level Mean F1 score is : 1.0\n",
            "34\n",
            "label starts with I! Impossible: from indices 198855\n",
            "Entity Level Mean F1 score is : 0.0\n",
            "35\n",
            "Entity Level Mean F1 score is : 0.0\n",
            "36\n",
            "Entity Level Mean F1 score is : 1.0\n",
            "37\n",
            "Entity Level Mean F1 score is : 1.0\n",
            "38\n",
            "Entity Level Mean F1 score is : 0.0\n",
            "39\n",
            "Entity Level Mean F1 score is : 0.0\n",
            "40\n",
            "Entity Level Mean F1 score is : 1.0\n",
            "41\n",
            "Entity Level Mean F1 score is : 0.0\n",
            "42\n",
            "label starts with I! Impossible: from indices 156946\n",
            "Entity Level Mean F1 score is : 0.0\n",
            "43\n",
            "Entity Level Mean F1 score is : 1.0\n",
            "44\n",
            "Entity Level Mean F1 score is : 0.4\n",
            "45\n",
            "Entity Level Mean F1 score is : 0.0\n",
            "46\n",
            "Entity Level Mean F1 score is : 0.5\n",
            "47\n",
            "Entity Level Mean F1 score is : 0.8\n",
            "48\n",
            "Entity Level Mean F1 score is : 1.0\n",
            "49\n",
            "Entity Level Mean F1 score is : 1.0\n",
            "50\n",
            "Entity Level Mean F1 score is : 0.0\n",
            "51\n",
            "Entity Level Mean F1 score is : 1.0\n",
            "52\n",
            "Entity Level Mean F1 score is : 1.0\n",
            "53\n",
            "Entity Level Mean F1 score is : 0.0\n",
            "54\n",
            "Entity Level Mean F1 score is : 0.0\n",
            "55\n",
            "label starts with I! Impossible: from indices 246746\n",
            "Entity Level Mean F1 score is : 1.0\n",
            "56\n",
            "Entity Level Mean F1 score is : 1.0\n",
            "57\n",
            "Entity Level Mean F1 score is : 0.5\n",
            "58\n",
            "Entity Level Mean F1 score is : 1.0\n",
            "59\n",
            "Entity Level Mean F1 score is : 0.0\n",
            "60\n",
            "label starts with I! Impossible: from indices 51944\n",
            "Entity Level Mean F1 score is : 0.0\n",
            "61\n",
            "Entity Level Mean F1 score is : 0.0\n",
            "62\n",
            "Entity Level Mean F1 score is : 0.0\n",
            "63\n",
            "Entity Level Mean F1 score is : 0.0\n",
            "64\n",
            "Entity Level Mean F1 score is : 1.0\n",
            "65\n",
            "Entity Level Mean F1 score is : 0.0\n",
            "66\n",
            "Entity Level Mean F1 score is : 1.0\n",
            "67\n",
            "Entity Level Mean F1 score is : 0.0\n",
            "68\n",
            "Entity Level Mean F1 score is : 1.0\n",
            "69\n",
            "Entity Level Mean F1 score is : 0.6666666666666666\n",
            "70\n",
            "Entity Level Mean F1 score is : 0.3333333333333333\n",
            "71\n",
            "Entity Level Mean F1 score is : 0.0\n",
            "72\n",
            "label starts with I! Impossible: from indices 77603\n",
            "label starts with I! Impossible: from indices 77606\n",
            "Entity Level Mean F1 score is : 0.0\n",
            "73\n",
            "Entity Level Mean F1 score is : 1.0\n",
            "74\n",
            "label starts with I! Impossible: from indices 83645\n",
            "label starts with I! Impossible: from indices 83642\n",
            "Entity Level Mean F1 score is : 0.14285714285714288\n",
            "75\n",
            "Entity Level Mean F1 score is : 0.0\n",
            "76\n",
            "label starts with I! Impossible: from indices 54798\n",
            "Entity Level Mean F1 score is : 0.0\n",
            "77\n",
            "Entity Level Mean F1 score is : 1.0\n",
            "78\n",
            "Entity Level Mean F1 score is : 0.0\n",
            "79\n",
            "Entity Level Mean F1 score is : 0.09523809523809525\n",
            "80\n",
            "Entity Level Mean F1 score is : 1.0\n",
            "81\n",
            "Entity Level Mean F1 score is : 0.0\n",
            "82\n",
            "Entity Level Mean F1 score is : 0.0\n",
            "83\n",
            "Entity Level Mean F1 score is : 0.0\n",
            "84\n",
            "Entity Level Mean F1 score is : 0.2222222222222222\n",
            "85\n",
            "Entity Level Mean F1 score is : 1.0\n",
            "86\n",
            "Entity Level Mean F1 score is : 0.0\n",
            "87\n",
            "Entity Level Mean F1 score is : 0.0\n",
            "88\n",
            "Entity Level Mean F1 score is : 1.0\n",
            "89\n",
            "Entity Level Mean F1 score is : 0.0\n",
            "90\n",
            "Entity Level Mean F1 score is : 0.0\n",
            "91\n",
            "Entity Level Mean F1 score is : 0.0\n",
            "92\n",
            "Entity Level Mean F1 score is : 0.0\n",
            "93\n",
            "Entity Level Mean F1 score is : 1.0\n",
            "94\n",
            "label starts with I! Impossible: from indices 79343\n",
            "label starts with I! Impossible: from indices 79346\n",
            "label starts with I! Impossible: from indices 79342\n",
            "label starts with I! Impossible: from indices 79345\n",
            "label starts with I! Impossible: from indices 79349\n",
            "Entity Level Mean F1 score is : 0.0\n",
            "95\n",
            "label starts with I! Impossible: from indices 246097\n",
            "Entity Level Mean F1 score is : 1.0\n",
            "96\n",
            "label starts with I! Impossible: from indices 159160\n",
            "Entity Level Mean F1 score is : 0.0\n",
            "97\n",
            "Entity Level Mean F1 score is : 0.0\n",
            "98\n",
            "Entity Level Mean F1 score is : 0.0\n",
            "99\n",
            "Entity Level Mean F1 score is : 0.0\n",
            "100\n",
            "label starts with I! Impossible: from indices 129408\n",
            "label starts with I! Impossible: from indices 129408\n",
            "Entity Level Mean F1 score is : 1.0\n",
            "101\n",
            "Entity Level Mean F1 score is : 0.0\n",
            "102\n",
            "Entity Level Mean F1 score is : 0.0\n",
            "103\n",
            "Entity Level Mean F1 score is : 1.0\n",
            "104\n",
            "Entity Level Mean F1 score is : 1.0\n",
            "105\n",
            "Entity Level Mean F1 score is : 1.0\n",
            "106\n",
            "label starts with I! Impossible: from indices 165361\n",
            "label starts with I! Impossible: from indices 165377\n",
            "label starts with I! Impossible: from indices 165377\n",
            "Entity Level Mean F1 score is : 0.0\n",
            "107\n",
            "Entity Level Mean F1 score is : 0.0\n",
            "108\n",
            "Entity Level Mean F1 score is : 1.0\n",
            "109\n",
            "label starts with I! Impossible: from indices 88958\n",
            "Entity Level Mean F1 score is : 0.0\n",
            "110\n",
            "Entity Level Mean F1 score is : 0.6666666666666666\n",
            "111\n",
            "Entity Level Mean F1 score is : 0.0\n",
            "112\n",
            "Entity Level Mean F1 score is : 0.0\n",
            "113\n",
            "Entity Level Mean F1 score is : 0.0\n",
            "114\n",
            "Entity Level Mean F1 score is : 0.5\n",
            "115\n",
            "label starts with I! Impossible: from indices 118824\n",
            "Entity Level Mean F1 score is : 0.5333333333333333\n",
            "116\n",
            "Entity Level Mean F1 score is : 0.0\n",
            "117\n",
            "Entity Level Mean F1 score is : 0.0\n",
            "118\n",
            "Entity Level Mean F1 score is : 0.0\n",
            "119\n",
            "Entity Level Mean F1 score is : 0.0\n",
            "120\n",
            "label starts with I! Impossible: from indices 7427\n",
            "Entity Level Mean F1 score is : 1.0\n",
            "121\n",
            "Entity Level Mean F1 score is : 1.0\n",
            "122\n",
            "Entity Level Mean F1 score is : 0.6666666666666665\n",
            "123\n",
            "Entity Level Mean F1 score is : 0.0\n",
            "124\n",
            "Entity Level Mean F1 score is : 1.0\n",
            "125\n",
            "Entity Level Mean F1 score is : 1.0\n",
            "126\n",
            "label starts with I! Impossible: from indices 267014\n",
            "Entity Level Mean F1 score is : 0.0\n",
            "127\n",
            "label starts with I! Impossible: from indices 41893\n",
            "Entity Level Mean F1 score is : 0.0\n",
            "128\n",
            "Entity Level Mean F1 score is : 0.0\n",
            "129\n",
            "Entity Level Mean F1 score is : 0.0\n",
            "130\n",
            "label starts with I! Impossible: from indices 160962\n",
            "Entity Level Mean F1 score is : 0.75\n",
            "131\n",
            "Entity Level Mean F1 score is : 1.0\n",
            "132\n",
            "Entity Level Mean F1 score is : 1.0\n",
            "133\n",
            "Entity Level Mean F1 score is : 0.5\n",
            "134\n",
            "Entity Level Mean F1 score is : 0.8\n",
            "135\n",
            "Entity Level Mean F1 score is : 0.0\n",
            "136\n",
            "label starts with I! Impossible: from indices 237619\n",
            "Entity Level Mean F1 score is : 0.0\n",
            "137\n",
            "Entity Level Mean F1 score is : 1.0\n",
            "138\n",
            "Entity Level Mean F1 score is : 0.0\n",
            "139\n",
            "Entity Level Mean F1 score is : 1.0\n",
            "140\n",
            "Entity Level Mean F1 score is : 0.5\n",
            "141\n",
            "Entity Level Mean F1 score is : 0.0\n",
            "142\n",
            "label starts with I! Impossible: from indices 19198\n",
            "Entity Level Mean F1 score is : 0.6666666666666666\n",
            "143\n",
            "Entity Level Mean F1 score is : 1.0\n",
            "144\n",
            "Entity Level Mean F1 score is : 0.0\n",
            "145\n",
            "Entity Level Mean F1 score is : 0.0\n",
            "146\n",
            "label starts with I! Impossible: from indices 39391\n",
            "label starts with I! Impossible: from indices 39395\n",
            "Entity Level Mean F1 score is : 0.6666666666666666\n",
            "147\n",
            "Entity Level Mean F1 score is : 0.0\n",
            "148\n",
            "Entity Level Mean F1 score is : 1.0\n",
            "149\n",
            "label starts with I! Impossible: from indices 50049\n",
            "Entity Level Mean F1 score is : 0.0\n",
            "150\n",
            "Entity Level Mean F1 score is : 0.0\n",
            "151\n",
            "Entity Level Mean F1 score is : 1.0\n",
            "152\n",
            "Entity Level Mean F1 score is : 0.0\n",
            "153\n",
            "label starts with I! Impossible: from indices 227977\n",
            "label starts with I! Impossible: from indices 227979\n",
            "Entity Level Mean F1 score is : 0.5\n",
            "154\n",
            "Entity Level Mean F1 score is : 0.0\n",
            "155\n",
            "Entity Level Mean F1 score is : 0.0\n",
            "156\n",
            "Entity Level Mean F1 score is : 0.0\n",
            "157\n",
            "label starts with I! Impossible: from indices 146110\n",
            "Entity Level Mean F1 score is : 0.0\n",
            "158\n",
            "Entity Level Mean F1 score is : 0.0\n",
            "159\n",
            "Entity Level Mean F1 score is : 0.0\n",
            "160\n",
            "Entity Level Mean F1 score is : 1.0\n",
            "161\n",
            "Entity Level Mean F1 score is : 1.0\n",
            "162\n",
            "label starts with I! Impossible: from indices 128040\n",
            "Entity Level Mean F1 score is : 0.0\n",
            "163\n",
            "Entity Level Mean F1 score is : 1.0\n",
            "164\n",
            "Entity Level Mean F1 score is : 0.923076923076923\n",
            "165\n",
            "Entity Level Mean F1 score is : 1.0\n",
            "166\n",
            "Entity Level Mean F1 score is : 1.0\n",
            "167\n",
            "Entity Level Mean F1 score is : 0.0\n",
            "168\n",
            "Entity Level Mean F1 score is : 0.0\n",
            "169\n",
            "Entity Level Mean F1 score is : 0.0\n",
            "170\n",
            "Entity Level Mean F1 score is : 0.0\n",
            "171\n",
            "Entity Level Mean F1 score is : 0.0\n",
            "172\n",
            "label starts with I! Impossible: from indices 251108\n",
            "Entity Level Mean F1 score is : 0.25\n",
            "173\n",
            "Entity Level Mean F1 score is : 0.0\n",
            "174\n",
            "Entity Level Mean F1 score is : 1.0\n",
            "175\n",
            "Entity Level Mean F1 score is : 0.5\n",
            "176\n",
            "Entity Level Mean F1 score is : 0.0\n",
            "177\n",
            "Entity Level Mean F1 score is : 0.0\n",
            "178\n",
            "label starts with I! Impossible: from indices 75057\n",
            "Entity Level Mean F1 score is : 0.0\n",
            "179\n",
            "Entity Level Mean F1 score is : 1.0\n",
            "180\n",
            "Entity Level Mean F1 score is : 1.0\n",
            "181\n",
            "Entity Level Mean F1 score is : 0.5\n",
            "182\n",
            "Entity Level Mean F1 score is : 1.0\n",
            "183\n",
            "Entity Level Mean F1 score is : 0.0\n",
            "184\n",
            "Entity Level Mean F1 score is : 1.0\n",
            "185\n",
            "Entity Level Mean F1 score is : 0.0\n",
            "186\n",
            "Entity Level Mean F1 score is : 0.07692307692307691\n",
            "187\n",
            "Entity Level Mean F1 score is : 0.0\n",
            "188\n",
            "Entity Level Mean F1 score is : 0.0\n",
            "189\n",
            "label starts with I! Impossible: from indices 266306\n",
            "Entity Level Mean F1 score is : 0.0\n",
            "190\n",
            "Entity Level Mean F1 score is : 0.0\n",
            "191\n",
            "Entity Level Mean F1 score is : 1.0\n",
            "192\n",
            "Entity Level Mean F1 score is : 0.0\n",
            "193\n",
            "Entity Level Mean F1 score is : 0.0\n",
            "194\n",
            "Entity Level Mean F1 score is : 0.5\n",
            "195\n",
            "label starts with I! Impossible: from indices 124571\n",
            "Entity Level Mean F1 score is : 0.5\n",
            "196\n",
            "Entity Level Mean F1 score is : 0.0\n",
            "197\n",
            "Entity Level Mean F1 score is : 0.0\n",
            "198\n",
            "Entity Level Mean F1 score is : 0.0\n",
            "199\n",
            "Entity Level Mean F1 score is : 0.0\n",
            "200\n",
            "label starts with I! Impossible: from indices 178820\n",
            "Entity Level Mean F1 score is : 0.0\n",
            "201\n",
            "Entity Level Mean F1 score is : 1.0\n",
            "202\n",
            "Entity Level Mean F1 score is : 0.0\n",
            "203\n",
            "Entity Level Mean F1 score is : 0.5\n",
            "204\n",
            "Entity Level Mean F1 score is : 0.0\n",
            "205\n",
            "Entity Level Mean F1 score is : 0.0\n",
            "206\n",
            "label starts with I! Impossible: from indices 199441\n",
            "label starts with I! Impossible: from indices 199445\n",
            "Entity Level Mean F1 score is : 0.0\n",
            "207\n",
            "Entity Level Mean F1 score is : 0.0\n",
            "208\n",
            "label starts with I! Impossible: from indices 51314\n",
            "Entity Level Mean F1 score is : 1.0\n",
            "209\n",
            "Entity Level Mean F1 score is : 0.0\n",
            "210\n",
            "Entity Level Mean F1 score is : 0.0\n",
            "211\n",
            "Entity Level Mean F1 score is : 0.0\n",
            "212\n",
            "label starts with I! Impossible: from indices 168468\n",
            "Entity Level Mean F1 score is : 0.0\n",
            "213\n",
            "Entity Level Mean F1 score is : 0.0\n",
            "214\n",
            "label starts with I! Impossible: from indices 179960\n",
            "Entity Level Mean F1 score is : 0.5\n",
            "215\n",
            "Entity Level Mean F1 score is : 0.6666666666666666\n",
            "216\n",
            "Entity Level Mean F1 score is : 0.6666666666666666\n",
            "217\n",
            "Entity Level Mean F1 score is : 1.0\n",
            "218\n",
            "Entity Level Mean F1 score is : 1.0\n",
            "219\n",
            "label starts with I! Impossible: from indices 25373\n",
            "Entity Level Mean F1 score is : 1.0\n",
            "220\n",
            "Entity Level Mean F1 score is : 0.0\n",
            "221\n",
            "label starts with I! Impossible: from indices 251587\n",
            "label starts with I! Impossible: from indices 251599\n",
            "label starts with I! Impossible: from indices 251599\n",
            "Entity Level Mean F1 score is : 0.75\n",
            "222\n",
            "Entity Level Mean F1 score is : 0.0\n",
            "223\n",
            "Entity Level Mean F1 score is : 0.0\n",
            "224\n",
            "Entity Level Mean F1 score is : 1.0\n",
            "225\n",
            "Entity Level Mean F1 score is : 0.0\n",
            "226\n",
            "Entity Level Mean F1 score is : 1.0\n",
            "227\n",
            "label starts with I! Impossible: from indices 229474\n",
            "Entity Level Mean F1 score is : 0.0\n",
            "228\n",
            "Entity Level Mean F1 score is : 1.0\n",
            "229\n",
            "Entity Level Mean F1 score is : 1.0\n",
            "230\n",
            "Entity Level Mean F1 score is : 0.0\n",
            "231\n",
            "Entity Level Mean F1 score is : 0.0\n",
            "232\n",
            "Entity Level Mean F1 score is : 1.0\n",
            "233\n",
            "Entity Level Mean F1 score is : 0.0\n",
            "234\n",
            "label starts with I! Impossible: from indices 197327\n",
            "Entity Level Mean F1 score is : 0.0\n",
            "235\n",
            "Entity Level Mean F1 score is : 0.0\n",
            "236\n",
            "Entity Level Mean F1 score is : 0.0\n",
            "237\n",
            "Entity Level Mean F1 score is : 0.0\n",
            "238\n",
            "Entity Level Mean F1 score is : 1.0\n",
            "239\n",
            "Entity Level Mean F1 score is : 0.0\n",
            "240\n",
            "Entity Level Mean F1 score is : 1.0\n",
            "241\n",
            "Entity Level Mean F1 score is : 1.0\n",
            "242\n",
            "Entity Level Mean F1 score is : 0.0\n",
            "243\n",
            "Entity Level Mean F1 score is : 0.0\n",
            "244\n",
            "label starts with I! Impossible: from indices 248554\n",
            "Entity Level Mean F1 score is : 0.0\n",
            "245\n",
            "label starts with I! Impossible: from indices 6365\n",
            "Entity Level Mean F1 score is : 0.5\n",
            "246\n",
            "Entity Level Mean F1 score is : 0.0\n",
            "247\n",
            "Entity Level Mean F1 score is : 1.0\n",
            "248\n",
            "label starts with I! Impossible: from indices 182843\n",
            "Entity Level Mean F1 score is : 0.6666666666666666\n",
            "249\n",
            "Entity Level Mean F1 score is : 0.8571428571428571\n",
            "250\n",
            "label starts with I! Impossible: from indices 86205\n",
            "Entity Level Mean F1 score is : 0.5\n",
            "251\n",
            "Entity Level Mean F1 score is : 0.0\n",
            "252\n",
            "Entity Level Mean F1 score is : 1.0\n",
            "253\n",
            "Entity Level Mean F1 score is : 1.0\n",
            "254\n",
            "Entity Level Mean F1 score is : 0.0\n",
            "255\n",
            "Entity Level Mean F1 score is : 1.0\n",
            "256\n",
            "Entity Level Mean F1 score is : 1.0\n",
            "257\n",
            "Entity Level Mean F1 score is : 0.0\n",
            "258\n",
            "Entity Level Mean F1 score is : 0.0\n",
            "259\n",
            "label starts with I! Impossible: from indices 121956\n",
            "Entity Level Mean F1 score is : 0.0\n",
            "260\n",
            "label starts with I! Impossible: from indices 122692\n",
            "Entity Level Mean F1 score is : 0.5\n",
            "261\n",
            "Entity Level Mean F1 score is : 0.0\n",
            "262\n",
            "Entity Level Mean F1 score is : 0.0\n",
            "263\n",
            "Entity Level Mean F1 score is : 1.0\n",
            "264\n",
            "Entity Level Mean F1 score is : 1.0\n",
            "265\n",
            "label starts with I! Impossible: from indices 230785\n",
            "Entity Level Mean F1 score is : 0.0\n",
            "266\n",
            "label starts with I! Impossible: from indices 154937\n",
            "Entity Level Mean F1 score is : 0.0\n",
            "267\n",
            "Entity Level Mean F1 score is : 1.0\n",
            "268\n",
            "Entity Level Mean F1 score is : 0.0\n",
            "269\n",
            "Entity Level Mean F1 score is : 0.0\n",
            "270\n",
            "Entity Level Mean F1 score is : 0.6666666666666666\n",
            "271\n",
            "Entity Level Mean F1 score is : 0.0\n",
            "272\n",
            "Entity Level Mean F1 score is : 0.0\n",
            "273\n",
            "label starts with I! Impossible: from indices 215313\n",
            "label starts with I! Impossible: from indices 215316\n",
            "Entity Level Mean F1 score is : 0.0\n",
            "274\n",
            "Entity Level Mean F1 score is : 0.0\n",
            "275\n",
            "label starts with I! Impossible: from indices 42504\n",
            "Entity Level Mean F1 score is : 0.0\n",
            "276\n",
            "label starts with I! Impossible: from indices 45112\n",
            "Entity Level Mean F1 score is : 0.0\n",
            "277\n",
            "Entity Level Mean F1 score is : 0.0\n",
            "278\n",
            "Entity Level Mean F1 score is : 1.0\n",
            "279\n",
            "Entity Level Mean F1 score is : 1.0\n",
            "280\n",
            "label starts with I! Impossible: from indices 230058\n",
            "Entity Level Mean F1 score is : 0.0\n",
            "281\n",
            "Entity Level Mean F1 score is : 0.0\n",
            "282\n",
            "Entity Level Mean F1 score is : 1.0\n",
            "283\n",
            "Entity Level Mean F1 score is : 0.0\n",
            "284\n",
            "Entity Level Mean F1 score is : 1.0\n",
            "285\n",
            "Entity Level Mean F1 score is : 0.0\n",
            "286\n",
            "Entity Level Mean F1 score is : 0.0\n",
            "287\n",
            "Entity Level Mean F1 score is : 0.0\n",
            "288\n",
            "Entity Level Mean F1 score is : 0.5\n",
            "289\n",
            "Entity Level Mean F1 score is : 1.0\n",
            "290\n",
            "label starts with I! Impossible: from indices 201795\n",
            "Entity Level Mean F1 score is : 0.0\n",
            "291\n",
            "label starts with I! Impossible: from indices 35121\n",
            "Entity Level Mean F1 score is : 0.0\n",
            "292\n",
            "Entity Level Mean F1 score is : 1.0\n",
            "293\n",
            "Entity Level Mean F1 score is : 0.0\n",
            "294\n",
            "Entity Level Mean F1 score is : 1.0\n",
            "295\n",
            "Entity Level Mean F1 score is : 0.5\n",
            "296\n",
            "Entity Level Mean F1 score is : 0.0\n",
            "297\n",
            "Entity Level Mean F1 score is : 0.0\n",
            "298\n",
            "Entity Level Mean F1 score is : 0.0\n",
            "299\n",
            "Entity Level Mean F1 score is : 1.0\n",
            "300\n",
            "label starts with I! Impossible: from indices 2394\n",
            "Entity Level Mean F1 score is : 0.0\n",
            "301\n",
            "label starts with I! Impossible: from indices 63016\n",
            "Entity Level Mean F1 score is : 1.0\n",
            "302\n",
            "label starts with I! Impossible: from indices 219552\n",
            "Entity Level Mean F1 score is : 0.3333333333333333\n",
            "303\n",
            "Entity Level Mean F1 score is : 0.3333333333333333\n",
            "304\n",
            "Entity Level Mean F1 score is : 0.0\n",
            "305\n",
            "Entity Level Mean F1 score is : 1.0\n",
            "306\n",
            "label starts with I! Impossible: from indices 86526\n",
            "label starts with I! Impossible: from indices 86549\n",
            "Entity Level Mean F1 score is : 0.0\n",
            "307\n",
            "Entity Level Mean F1 score is : 0.0\n",
            "308\n",
            "label starts with I! Impossible: from indices 47939\n",
            "label starts with I! Impossible: from indices 47954\n",
            "Entity Level Mean F1 score is : 0.8888888888888888\n",
            "309\n",
            "Entity Level Mean F1 score is : 0.0\n",
            "310\n",
            "Entity Level Mean F1 score is : 0.6666666666666666\n",
            "311\n",
            "Entity Level Mean F1 score is : 1.0\n",
            "312\n",
            "Entity Level Mean F1 score is : 0.0\n",
            "313\n",
            "Entity Level Mean F1 score is : 1.0\n",
            "314\n",
            "Entity Level Mean F1 score is : 1.0\n",
            "315\n",
            "Entity Level Mean F1 score is : 0.0\n",
            "316\n",
            "label starts with I! Impossible: from indices 38726\n",
            "Entity Level Mean F1 score is : 0.0\n",
            "317\n",
            "label starts with I! Impossible: from indices 159266\n",
            "Entity Level Mean F1 score is : 0.25\n",
            "318\n",
            "Entity Level Mean F1 score is : 0.0\n",
            "319\n",
            "label starts with I! Impossible: from indices 216616\n",
            "Entity Level Mean F1 score is : 0.0\n",
            "320\n",
            "Entity Level Mean F1 score is : 0.0\n",
            "321\n",
            "Entity Level Mean F1 score is : 0.0\n",
            "322\n",
            "Entity Level Mean F1 score is : 0.0\n",
            "323\n",
            "Entity Level Mean F1 score is : 0.0\n",
            "324\n",
            "label starts with I! Impossible: from indices 19175\n",
            "Entity Level Mean F1 score is : 0.0\n",
            "325\n",
            "Entity Level Mean F1 score is : 0.6666666666666666\n",
            "326\n",
            "Entity Level Mean F1 score is : 0.5\n",
            "327\n",
            "Entity Level Mean F1 score is : 1.0\n",
            "328\n",
            "label starts with I! Impossible: from indices 102793\n",
            "Entity Level Mean F1 score is : 0.6666666666666666\n",
            "329\n",
            "label starts with I! Impossible: from indices 76233\n",
            "label starts with I! Impossible: from indices 76231\n",
            "Entity Level Mean F1 score is : 0.4\n",
            "330\n",
            "Entity Level Mean F1 score is : 0.0\n",
            "331\n",
            "Entity Level Mean F1 score is : 1.0\n",
            "332\n",
            "Entity Level Mean F1 score is : 1.0\n",
            "333\n",
            "Entity Level Mean F1 score is : 1.0\n",
            "334\n",
            "label starts with I! Impossible: from indices 10809\n",
            "label starts with I! Impossible: from indices 10811\n",
            "label starts with I! Impossible: from indices 10814\n",
            "Entity Level Mean F1 score is : 0.0\n",
            "335\n",
            "Entity Level Mean F1 score is : 0.5\n",
            "336\n",
            "Entity Level Mean F1 score is : 0.6666666666666666\n",
            "337\n",
            "Entity Level Mean F1 score is : 0.0\n",
            "338\n",
            "Entity Level Mean F1 score is : 0.0\n",
            "339\n",
            "Entity Level Mean F1 score is : 1.0\n",
            "340\n",
            "Entity Level Mean F1 score is : 1.0\n",
            "341\n",
            "label starts with I! Impossible: from indices 106177\n",
            "Entity Level Mean F1 score is : 0.0\n",
            "342\n",
            "Entity Level Mean F1 score is : 0.0\n",
            "343\n",
            "Entity Level Mean F1 score is : 0.0\n",
            "344\n",
            "Entity Level Mean F1 score is : 1.0\n",
            "345\n",
            "Entity Level Mean F1 score is : 1.0\n",
            "346\n",
            "Entity Level Mean F1 score is : 1.0\n",
            "347\n",
            "label starts with I! Impossible: from indices 48662\n",
            "Entity Level Mean F1 score is : 0.4\n",
            "348\n",
            "label starts with I! Impossible: from indices 181226\n",
            "Entity Level Mean F1 score is : 0.6666666666666666\n",
            "349\n",
            "label starts with I! Impossible: from indices 180853\n",
            "Entity Level Mean F1 score is : 0.5\n",
            "350\n",
            "Entity Level Mean F1 score is : 1.0\n",
            "351\n",
            "Entity Level Mean F1 score is : 1.0\n",
            "352\n",
            "Entity Level Mean F1 score is : 1.0\n",
            "353\n",
            "label starts with I! Impossible: from indices 225569\n",
            "Entity Level Mean F1 score is : 0.0\n",
            "354\n",
            "label starts with I! Impossible: from indices 217740\n",
            "Entity Level Mean F1 score is : 0.0\n",
            "355\n",
            "Entity Level Mean F1 score is : 0.0\n",
            "356\n",
            "Entity Level Mean F1 score is : 0.0\n",
            "357\n",
            "Entity Level Mean F1 score is : 0.0\n",
            "358\n",
            "label starts with I! Impossible: from indices 116514\n",
            "Entity Level Mean F1 score is : 0.0\n",
            "359\n",
            "Entity Level Mean F1 score is : 1.0\n",
            "360\n",
            "Entity Level Mean F1 score is : 0.0\n",
            "361\n",
            "Entity Level Mean F1 score is : 0.6666666666666666\n",
            "362\n",
            "Entity Level Mean F1 score is : 0.0\n",
            "363\n",
            "Entity Level Mean F1 score is : 1.0\n",
            "364\n",
            "label starts with I! Impossible: from indices 953\n",
            "Entity Level Mean F1 score is : 0.0\n",
            "365\n",
            "Entity Level Mean F1 score is : 0.6666666666666666\n",
            "366\n",
            "Entity Level Mean F1 score is : 0.0\n",
            "367\n",
            "Entity Level Mean F1 score is : 0.4444444444444445\n",
            "368\n",
            "Entity Level Mean F1 score is : 0.0\n",
            "369\n",
            "Entity Level Mean F1 score is : 1.0\n",
            "370\n",
            "Entity Level Mean F1 score is : 0.0\n",
            "371\n",
            "Entity Level Mean F1 score is : 1.0\n",
            "372\n",
            "Entity Level Mean F1 score is : 0.0\n",
            "373\n",
            "Entity Level Mean F1 score is : 0.0\n",
            "374\n",
            "Entity Level Mean F1 score is : 0.0\n",
            "375\n",
            "Entity Level Mean F1 score is : 1.0\n",
            "376\n",
            "label starts with I! Impossible: from indices 205913\n",
            "label starts with I! Impossible: from indices 205923\n",
            "Entity Level Mean F1 score is : 0.0\n",
            "377\n",
            "Entity Level Mean F1 score is : 0.4\n",
            "378\n",
            "Entity Level Mean F1 score is : 0.0\n",
            "379\n",
            "label starts with I! Impossible: from indices 99185\n",
            "label starts with I! Impossible: from indices 99187\n",
            "Entity Level Mean F1 score is : 0.0\n",
            "380\n",
            "Entity Level Mean F1 score is : 0.0\n",
            "381\n",
            "Entity Level Mean F1 score is : 1.0\n",
            "382\n",
            "Entity Level Mean F1 score is : 0.0\n",
            "383\n",
            "Entity Level Mean F1 score is : 0.0\n",
            "384\n",
            "Entity Level Mean F1 score is : 0.0\n",
            "385\n",
            "Entity Level Mean F1 score is : 1.0\n",
            "386\n",
            "Entity Level Mean F1 score is : 1.0\n",
            "387\n",
            "Entity Level Mean F1 score is : 1.0\n",
            "388\n",
            "Entity Level Mean F1 score is : 0.0\n",
            "389\n",
            "Entity Level Mean F1 score is : 0.0\n",
            "390\n",
            "label starts with I! Impossible: from indices 199336\n",
            "label starts with I! Impossible: from indices 199339\n",
            "Entity Level Mean F1 score is : 0.0\n",
            "391\n",
            "Entity Level Mean F1 score is : 0.0\n",
            "392\n",
            "Entity Level Mean F1 score is : 0.0\n",
            "393\n",
            "Entity Level Mean F1 score is : 1.0\n",
            "394\n",
            "Entity Level Mean F1 score is : 0.0\n",
            "395\n",
            "label starts with I! Impossible: from indices 177987\n",
            "Entity Level Mean F1 score is : 0.0\n",
            "396\n",
            "label starts with I! Impossible: from indices 256041\n",
            "Entity Level Mean F1 score is : 0.0\n",
            "397\n",
            "Entity Level Mean F1 score is : 0.0\n",
            "398\n",
            "label starts with I! Impossible: from indices 38216\n",
            "Entity Level Mean F1 score is : 0.0\n",
            "399\n",
            "label starts with I! Impossible: from indices 110952\n",
            "Entity Level Mean F1 score is : 0.3333333333333333\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fGkhL1imxpmH"
      },
      "source": [
        "<a name=\"q3.1\"></a>\n",
        "[[^^^]](#outline) \n",
        "\n",
        "## **Q3.1: Implementation Details**\n",
        "Explain how you implemented the MEMM and whether/how you modified Viterbi (e.g. which algorithms/data structures you used, what features are included). Make clear which parts were implemented from scratch vs. obtained via an existing package.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "etkqBEJxOZbI"
      },
      "source": [
        "#### **A3.1:**\n",
        "\n",
        "In total we have 5 functions that help implementation:\n",
        "  - pos_tag\n",
        "    - This function uses existing package \"nltk.pos_tag\" to get the tag sequence of the tokens\n",
        "  - extract_feature\n",
        "    - In this function, we define the features and return them in a map, we do not use package in this part\n",
        "  - join-feature\n",
        "    - In this function, we join each the feature and label of the sequence in a tuple, and return the new list that could input to the classifier. No package for his too.\n",
        "  - memm\n",
        "    - In this function, we called pos_tag to tag the tokens and called extract_features and join_features to get the input and then use it to train the classifier, we then wrap up all of the things in return. The package we use is nltk.classify.MaxentClassifier\n",
        "  - vertibi-memm\n",
        "    - Pretty similar compared with HMM. We first extract the features of observation list, and put it into the classifier.prob_classify to get the prob matrix for each label in a word. we then replace the whole transition and emission matrix in HMM, do the same vertibi as in HMM. We first define a dp List[List[int]], initialize it with the first observation probs and same data structure as backpointer. We then first loop the observation, second loop the pre label list and third loop the current label list, we track the backpointer as well. In the end, we start from the largest prob in dp, backtrack the pointer save in a list. Finally, we return the reverse list"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m_eDwiILvHGL"
      },
      "source": [
        "<a name=\"q3.2\"></a>\n",
        "[[^^^]](#outline) \n",
        "\n",
        "## **Q3.2: Results Analysis**\n",
        "Explain here how you evaluated the MEMM model. Summarize the performance of your system and any variations that you experimented with the validation datasets. Put the results into clearly labeled tables or diagrams and include your observations and analysis.\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(mean_mean_f1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BdRXXu0A6GI7",
        "outputId": "31631d53-e741-4e12-ac2f-22667767bc66"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.36908531746031753\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(variance(mean_f1_list))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iY0ff3jz6Hj0",
        "outputId": "0b9934fa-6f3e-451e-9eec-8795a36c2c22"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.19699416116990165\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "udTvSjp1ObVy"
      },
      "source": [
        "#### **A3.2:**\n",
        "\n",
        "We use F1 score to evaluated the model. when F1 = 1, the model can perfectly predict the BOI tag of each tokens in validation data (predicted BOI same as provided BOI). our mean f1 is about 0.3678 and variance is about 0.1963."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ammZn20RZn8h"
      },
      "source": [
        "<a name=\"q3.3\"></a>\n",
        "[[^^^]](#outline) \n",
        "\n",
        "## **Q3.3: Feature Engineering**\n",
        "What features are considered most important by your MaxEnt Classifier? Why do you think these features make sense? Describe your experiments with feature sets. An analysis on feature selection for the MEMM is required  e.g. what features **help most**, why? An **error analysis** is required  e.g. what sorts of errors occurred, why?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "foadgkcOOcvT"
      },
      "source": [
        "#### **A3.3:**\n",
        "\n",
        "the previous tokens and previous labels are the most important by our MaxEnt Classifier. In language grammar, the sequence of different types of words are important, for example, a noun normally shows after a verb or an article.\n",
        "\n",
        "the features we use to train the model are: 'is_title' to check if the first char in a word is upper form. 'cur_token' to check what is the token itself, which helps predicting when meet the same words next time. 'cur_pos' to check what is the current POS tag assigned. '1_prev_pos\" to check what is the POS tag assigned to the one previous the current word. '1_prev_token\" to check what is the word previous the current word.\n",
        "\n",
        "However, with this feature, MEMM model is tend to mis-predict \"B-MISC\". That is maybe because this is Miscellaneous entities and our features tend to work on more common entity such as person, organization etc. However, when our model meet \"B-MISC\", it may predict it as other entity or even \"O\"."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h4XaDMlcaDBA"
      },
      "source": [
        "<a name=\"q3.4\"></a>\n",
        "[[^^^]](#outline) \n",
        "\n",
        "## **Q3.4: Room for Improvement**\n",
        "When did the system work well, when did it fail and any ideas as to why? How might you improve the system? \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q_esMp_pOeGD"
      },
      "source": [
        "#### **A3.4:**\n",
        "\n",
        "the system work well when the entity is not Miscellaneous entities. it fails when the entity is not Miscellaneous entities.\n",
        "\n",
        "to improve the system, I would like to add more features that can check Miscellaneous entities, such as: if it is a city by using GeoText(cur_token).cities function."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lQwPwqU3vMiS"
      },
      "source": [
        "<a name=\"part4\"></a>\n",
        "# **Part 4: Comparing HMMs and MEMMs**\n",
        "[[^^^]](#outline) \n",
        "\n",
        "---\n",
        "\n",
        "In this section you will be asked to analyze and compare the models you have developed!\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ndVxVFzFagZ5"
      },
      "source": [
        "<a name=\"q4.1\"></a>\n",
        "[[^^^]](#outline) \n",
        "\n",
        "## **Q4.1: Result Comparison**\n",
        "Compare here your results (validation scores) for your HMM and the MEMM. Which of them performs better? Why?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FuXjldBAOgXf"
      },
      "source": [
        "#### **A4.1:**\n",
        "\n",
        "the validation score for HMM is 0.15 higher than the validation score for MEMM, which means HMM model is better. the reason is because 1. we need more training data set to increase the accuracy of features. 2. the chosen feature may not fit the semantic of validation and training data well."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cjGcdm5aafl3"
      },
      "source": [
        "<a name=\"q4.2\"></a>\n",
        "[[^^^]](#outline) \n",
        "\n",
        "## **Q4.2: Error Analysis 1**\n",
        "Do some error analysis. What are error patterns you observed that the HMM makes but the MEMM does not? Try to justify why/why not? **Please give examples from the dataset.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NzFPgH9MOiQw"
      },
      "source": [
        "#### **A4.2:**\n",
        "\n",
        "the error patterns where HMM makes is that when \"unknown\" words show, HMM model ususally sets it as \"O\" where MEMM model does not. That is because, to handle unknown words, rather than directly set the words \"unknown\", MEMM tends to assign it different feature. When next time meet a similar feature set in observation, MEMM tends to assign it same labels as the \"unknown\" word in training set.\n",
        "\n",
        "Here is the example from the second element in validation set:\n",
        "the text/tokens list is: ['As', 'the', 'UNK', 'UNK', 'track', \"'s\", '(', '\"', 'UNK', 'v', '.']\n",
        "\n",
        "and the given/true labels list: ['O', 'O', 'B-ORG', 'I-ORG', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
        "\n",
        "However, the predicted labels list comes to: ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rZYfZAgga9UB"
      },
      "source": [
        "<a name=\"q4.3\"></a>\n",
        "[[^^^]](#outline) \n",
        "\n",
        "## **Q4.3: Error Analysis 2**\n",
        "What are error patterns you observed that MEMM makes but the HMM does not? Try to justify what you observe? **Please give examples from the dataset.** "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Shm8ZaaoOjbg"
      },
      "source": [
        "#### **A4.3:**\n",
        "\n",
        "MEMM model is tend to mis-predict \"B-MISC\". That is maybe because this is Miscellaneous entities and our features tend to work on more common entity such as person, organization etc. However, when our model meet \"B-MISC\", it may predict it as other entity or even \"O\".\n",
        "\n",
        "Here is the example from the 20th element in validation set:\n",
        "the text/tokens list is: ['The','2007', 'IPCC', 'report', 'noted', 'many', 'observed', 'changes', 'in', 'the', 'climate', ',', 'including', 'atmospheric', 'composition', ',','global','average','temperatures', ',','ocean','conditions',',','and','others','.']\n",
        "\n",
        "and the given/true labels list: ['O','O', 'B-MISC', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
        "\n",
        "However, the predicted labels list comes to: ['O','O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TpugxBD7RBy1"
      },
      "source": [
        "<a name=\"part5\"></a>\n",
        "# **Part 5: Kaggle Submission**\n",
        "[[^^^]](#outline) \n",
        "\n",
        "---\n",
        "\n",
        "Using the best-performing system from among all of your HMM and MEMM models, generate predictions for the test set, and submit them to [Kaggle competition](https://www.kaggle.com/t/200697e4726f448986930dd4e823e957). Below, we provide a function that submits given predicted tokens and associated token indices in the correct format. As a scoring metric on Kaggle, we use **Entity Level Mean F1**.\n",
        "\n",
        "Your submission to Kaggle should be a CSV file consisting of five lines and two columns. The first line is a fixed header, and each of the remaining four lines corresponds to one of the four types of named entities. The first column is the label identifier *Id* (one of PER, LOC, ORG or MISC), and the second column *Predicted* is a list of entities (separated by single space) that you predict to be of that type. Each entity is specified by its starting and ending index (concatenated by a hypen) as given in the test corpus. \n",
        "\n",
        "You can use the function **create_submission** that takes the list of predicted labels and the list of associated token indices as inputs and creates the the output CSV file at a specified path.\n",
        "\n",
        "NOTE: Ensure that there are **no** rows with *Id* = \"O\" in your Kaggle Submission."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "893l9j77ETFM"
      },
      "outputs": [],
      "source": [
        "import csv\n",
        "\n",
        "def create_submission(output_filepath, token_labels, token_inds):\n",
        "    \"\"\"\n",
        "    :parameter output_filepath: The full path (including file name) of the output file, \n",
        "                                with extension .csv\n",
        "    :type output_filepath: [String]\n",
        "    :parameter token_labels: A list of token labels (eg. PER, LOC, ORG or MISC).\n",
        "    :type token_labels: List[String]\n",
        "    :parameter token_indices: A list of token indices (taken from the dataset) \n",
        "                              corresponding to the labels in [token_labels].\n",
        "    :type token_indices: List[int]\n",
        "    \"\"\"\n",
        "    label_dict = format_output_labels(token_labels, token_inds)\n",
        "    with open(output_filepath, mode='w') as csv_file:\n",
        "        fieldnames = ['Id', 'Predicted']\n",
        "        writer = csv.DictWriter(csv_file, fieldnames=fieldnames)\n",
        "        writer.writeheader()\n",
        "        for key in label_dict:\n",
        "            p_string = \" \".join([str(start)+\"-\"+str(end) for start,end in label_dict[key]])\n",
        "            writer.writerow({'Id': key, 'Predicted': p_string})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bVASi55a_kQA"
      },
      "outputs": [],
      "source": [
        "def flatten_list(_2d_list):\n",
        "    flat_list = []\n",
        "    # Iterate through the outer list\n",
        "    for element in _2d_list:\n",
        "        if type(element) is list:\n",
        "            # If the element is of type list, iterate through the sublist\n",
        "            for item in element:\n",
        "                flat_list.append(item)\n",
        "        else:\n",
        "            flat_list.append(element)\n",
        "    return flat_list"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bTofG9q06-xN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "066cb044-6420-4482-ba75-e5ea9b44ea0e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "label starts with I! Impossible: from indices 180893\n",
            "label starts with I! Impossible: from indices 13685\n",
            "label starts with I! Impossible: from indices 223527\n",
            "label starts with I! Impossible: from indices 3344\n",
            "label starts with I! Impossible: from indices 223442\n",
            "label starts with I! Impossible: from indices 106657\n",
            "label starts with I! Impossible: from indices 121331\n",
            "label starts with I! Impossible: from indices 205489\n",
            "label starts with I! Impossible: from indices 68347\n",
            "label starts with I! Impossible: from indices 114154\n"
          ]
        }
      ],
      "source": [
        "output_filepath = os.path.join(os.getcwd(), \"drive\", \"MyDrive\", \"Milestone\") + \".csv\"\n",
        "labels_lists = []\n",
        "for i in test_data['text']:\n",
        "  labels_lists.append(viterbi(hmm, i))\n",
        "token_labels = flatten_list(labels_lists)\n",
        "token_inds = flatten_list(test_data['index'])\n",
        "create_submission(output_filepath, token_labels, token_inds)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "qHW8m87AjtK9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A08K0mtH3FPo",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "886b0e89-b46a-4ee8-8510-55153445167b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "label starts with I! Impossible: from indices 221600\n",
            "label starts with I! Impossible: from indices 86624\n",
            "label starts with I! Impossible: from indices 161274\n",
            "label starts with I! Impossible: from indices 72963\n",
            "label starts with I! Impossible: from indices 235427\n",
            "label starts with I! Impossible: from indices 30851\n",
            "label starts with I! Impossible: from indices 84691\n",
            "label starts with I! Impossible: from indices 208628\n",
            "label starts with I! Impossible: from indices 208630\n",
            "label starts with I! Impossible: from indices 133458\n",
            "label starts with I! Impossible: from indices 137479\n",
            "label starts with I! Impossible: from indices 6136\n",
            "label starts with I! Impossible: from indices 229663\n",
            "label starts with I! Impossible: from indices 112789\n",
            "label starts with I! Impossible: from indices 138700\n",
            "label starts with I! Impossible: from indices 265683\n",
            "label starts with I! Impossible: from indices 29723\n",
            "label starts with I! Impossible: from indices 128470\n",
            "label starts with I! Impossible: from indices 246448\n",
            "label starts with I! Impossible: from indices 78322\n",
            "label starts with I! Impossible: from indices 165203\n",
            "label starts with I! Impossible: from indices 27835\n",
            "label starts with I! Impossible: from indices 138598\n",
            "label starts with I! Impossible: from indices 205846\n",
            "label starts with I! Impossible: from indices 134462\n",
            "label starts with I! Impossible: from indices 42487\n",
            "label starts with I! Impossible: from indices 95630\n",
            "label starts with I! Impossible: from indices 227731\n",
            "label starts with I! Impossible: from indices 190663\n",
            "label starts with I! Impossible: from indices 190676\n",
            "label starts with I! Impossible: from indices 142397\n",
            "label starts with I! Impossible: from indices 18537\n",
            "label starts with I! Impossible: from indices 237669\n",
            "label starts with I! Impossible: from indices 233244\n",
            "label starts with I! Impossible: from indices 233255\n",
            "label starts with I! Impossible: from indices 256699\n",
            "label starts with I! Impossible: from indices 3345\n",
            "label starts with I! Impossible: from indices 160621\n",
            "label starts with I! Impossible: from indices 10127\n",
            "label starts with I! Impossible: from indices 195171\n",
            "label starts with I! Impossible: from indices 31378\n",
            "label starts with I! Impossible: from indices 117021\n",
            "label starts with I! Impossible: from indices 121331\n",
            "label starts with I! Impossible: from indices 20525\n",
            "label starts with I! Impossible: from indices 225763\n",
            "label starts with I! Impossible: from indices 176887\n",
            "label starts with I! Impossible: from indices 15617\n",
            "label starts with I! Impossible: from indices 185675\n",
            "label starts with I! Impossible: from indices 200962\n",
            "label starts with I! Impossible: from indices 114154\n",
            "label starts with I! Impossible: from indices 143373\n",
            "label starts with I! Impossible: from indices 210710\n",
            "label starts with I! Impossible: from indices 869\n",
            "label starts with I! Impossible: from indices 171095\n",
            "label starts with I! Impossible: from indices 161740\n",
            "label starts with I! Impossible: from indices 57169\n",
            "label starts with I! Impossible: from indices 187210\n",
            "label starts with I! Impossible: from indices 181874\n",
            "label starts with I! Impossible: from indices 236315\n"
          ]
        }
      ],
      "source": [
        "output_filepath = os.path.join(os.getcwd(), \"drive\", \"MyDrive\", \"test_memm3\") + \".csv\"\n",
        "res= []\n",
        "for i in test_data['text']:\n",
        "  res.append(vertibi_memm(memm_model, i))\n",
        "flat_res= flatten_list(res)\n",
        "inds= flatten_list(test_data['index'])\n",
        "create_submission(output_filepath, flat_res, inds)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U1lNil41VqMn"
      },
      "source": [
        "## **Baselines**\n",
        "\n",
        "On Kaggle, we provide two baselines for you to evaluate your models agaist: **`HMM Baseline`** and **`MEMM Baseline`**. You may use them to internally check your models. In addition, you may get points if for the final submission your best-performing model does better than the **`MEMM baseline`**.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BEZP_FGivVRI"
      },
      "source": [
        "---\n",
        "<a name=\"q5\"></a>\n",
        "## **Q5: Competition Score**\n",
        "[[^^^]](#outline) \n",
        "\n",
        "\n",
        "Include your team's **best score** and the **name under which that best score was submitted** from Kaggle. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bYirLL5x1mE7"
      },
      "source": [
        "#### **A5:**\n",
        "\n",
        "the best score is 0.52252 and the name is Milestone-2.csv submitted by team \"god bless us\""
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "3YCo0Zx0KPdw"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}